[{"categories":["GitHub"],"content":"本文介绍如何在一台电脑上，配置多个 Git 的连接信息，同时连接 GitHub 和公司的 GitLab。 ","date":"2020-10-29","objectID":"/2020/10/multi-git-account/:0:0","tags":["ssh","git"],"title":"Mac 上配置多个 GitHub 的 ssh 账号","uri":"/2020/10/multi-git-account/"},{"categories":["GitHub"],"content":"一、前言 日常工作时我本机需要连接公司的 Gitlab，还需要连接两个 GitHub 账号，本地使用的时候免不了需要不同的 SSH 认证登录。 ","date":"2020-10-29","objectID":"/2020/10/multi-git-account/:1:0","tags":["ssh","git"],"title":"Mac 上配置多个 GitHub 的 ssh 账号","uri":"/2020/10/multi-git-account/"},{"categories":["GitHub"],"content":"二、如何配置 添加 ssh-agent $ eval \"$(ssh-agent -s)\" Agent pid 65799 查看初始化的认证信息 $ ssh-add -L The agent has no identities. 把 rsa 私钥添加到 ssh $ ssh-add ~/.ssh/id_rsa2 Identity added: /Users/tc/.ssh/id_rsa2 (/Users/tc/.ssh/id_rsa2) 查看添加后的认证信息 $ ssh-add -L ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCmz+FyF/uuIYAedTbzQoAE+wnHG3/uzqQ2Mv9QQbjcWOOjOlTmL48rYTH6hA1rJQl9hFsTrYzTbKYlV1h7oNqT/7nUsmNOhItg51DcvSbA/PSw+2jADkbMV6bnKUtJmx7gxu7RKuWi48smEewYny/zMIJSPRu5Tr1G6WX/zEtonJntgEJELfgrJntgBikTZ/QJOGtejwWJjEPyKyNji0O4o0tmK5Xql+MG0O109Xa5toRBADFWqz+Popk7MNFuooRp/Fz0b4tLTvvlfwrEGDu0z4qmlShTmiZNQhYQsmiwpt27/IUo2Cw8J62EEUqviykuNeZBzYCGMXd3Mcg/CpTPbtBb /Users/tc/.ssh/id_rsa2 查看 github.com 交互的账号信息 $ ssh -T git@github.com Hi cityiron! You've successfully authenticated, but GitHub does not provide shell access. 当出现 Hi [yourname]， name 和你要操作的仓库所在的 github 账号一样，那么就可以正常使用 Git 命令。 每一个 terminal 可以是独立的 session，当电脑启动的时候，默认所有的 terminal 共享一个账号信息 ","date":"2020-10-29","objectID":"/2020/10/multi-git-account/:2:0","tags":["ssh","git"],"title":"Mac 上配置多个 GitHub 的 ssh 账号","uri":"/2020/10/multi-git-account/"},{"categories":["GitHub"],"content":"三、参考资源 https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent ","date":"2020-10-29","objectID":"/2020/10/multi-git-account/:3:0","tags":["ssh","git"],"title":"Mac 上配置多个 GitHub 的 ssh 账号","uri":"/2020/10/multi-git-account/"},{"categories":["Gin"],"content":"本文介绍通过 Gin 框架访问后端的静态资源 ","date":"2020-10-23","objectID":"/2020/10/gin-static/:0:0","tags":["静态资源","React","Ant Design"],"title":"一拳搞定 Gin | 前后端分离下静态资源部署到后端访问","uri":"/2020/10/gin-static/"},{"categories":["Gin"],"content":"一、前言 最近给公司做运维平台，前端是基于 Ant Design Pro 构建的 React 项目，后端是基于 Gin 搭建的 Go Web 项目。在部署的时候考虑到方便性，选择了把构建好的前端资源放到后端服务器，通过启动后端服务器直接运行前后端程序。 ","date":"2020-10-23","objectID":"/2020/10/gin-static/:1:0","tags":["静态资源","React","Ant Design"],"title":"一拳搞定 Gin | 前后端分离下静态资源部署到后端访问","uri":"/2020/10/gin-static/"},{"categories":["Gin"],"content":"二、实战分析 ","date":"2020-10-23","objectID":"/2020/10/gin-static/:2:0","tags":["静态资源","React","Ant Design"],"title":"一拳搞定 Gin | 前后端分离下静态资源部署到后端访问","uri":"/2020/10/gin-static/"},{"categories":["Gin"],"content":"2.1 静态资源 代码： func main() { r := gin.Default() dir, _ := os.Getwd() // static r.Static(\"/assets\", dir+\"/src/gin/assets\") r.StaticFS(\"/more_static\", http.Dir(dir+\"/src/gin/assets\")) r.StaticFile(\"/favicon.ico\", dir+\"/src/gin/resources/favicon.ico\") // Listen and serve on 0.0.0.0:8080 r.Run(\":8080\") } 访问如下地址： http://127.0.0.1:8080/more_static/favicon.ico http://127.0.0.1:8080/assets/favicon.ico http://127.0.0.1:8080/favicon.ico 均可以请求到头像图片。 2.1.1 静态文件 访问某个路径，直接找到对应的资源返回。 使用： r.StaticFile(\"/favicon.ico\", dir+\"/src/gin/resources/favicon.ico\") 源码分析： // File writes the specified file into the body stream in a efficient way. func (c *Context) File(filepath string) { http.ServeFile(c.Writer, c.Request, filepath) } Gin 接收到的 HTTP 请求会到这里，然后交给 http 文件服务。 /net/http/fs.go:674 func ServeFile(w ResponseWriter, r *Request, name string) { if containsDotDot(r.URL.Path) { // Too many programs use r.URL.Path to construct the argument to // serveFile. Reject the request under the assumption that happened // here and \"..\" may not be wanted. // Note that name might not contain \"..\", for example if code (still // incorrectly) used filepath.Join(myDir, r.URL.Path). Error(w, \"invalid URL path\", StatusBadRequest) return } dir, file := filepath.Split(name) serveFile(w, r, Dir(dir), file, false) } 分隔名称，会分离出资源的路径和名称 然后调用寻找文件的逻辑 /net/http/fs.go:546 // name is '/'-separated, not filepath.Separator. func serveFile(w ResponseWriter, r *Request, fs FileSystem, name string, redirect bool) { const indexPage = \"/index.html\" // redirect .../index.html to .../ // can't use Redirect() because that would make the path absolute, // which would be a problem running under StripPrefix if strings.HasSuffix(r.URL.Path, indexPage) { localRedirect(w, r, \"./\") return } f, err := fs.Open(name) if err != nil { msg, code := toHTTPError(err) Error(w, msg, code) return } defer f.Close() d, err := f.Stat() if err != nil { msg, code := toHTTPError(err) Error(w, msg, code) return } ... } 用的 net 包的 filesystem 打开文件，如果不存在则包 HTTP 的 404 错误 如果是文件夹会找 index.html 文件，本文不扩展 func (d Dir) Open(name string) (File, error) { if filepath.Separator != '/' \u0026\u0026 strings.ContainsRune(name, filepath.Separator) { return nil, errors.New(\"http: invalid character in file path\") } dir := string(d) if dir == \"\" { dir = \".\" } fullName := filepath.Join(dir, filepath.FromSlash(path.Clean(\"/\"+name))) f, err := os.Open(fullName) if err != nil { return nil, mapDirOpenError(err, fullName) } return f, nil } 拼接完整的路径 使用标准的 os.Open() 打开 2.1.2 静态目录 路径加上某个前缀对应访问某个目录下的资源。 使用： r.Static(\"/assets\", dir+\"/src/gin/assets\") 源码分析： func (group *RouterGroup) Static(relativePath, root string) IRoutes { return group.StaticFS(relativePath, Dir(root, false)) } 实际调用也是 StaticFS 方法，加了个是否展示目录下的内容的控制 /github.com/gin-gonic/gin/fs.go:24 func Dir(root string, listDirectory bool) http.FileSystem { fs := http.Dir(root) if listDirectory { return fs } return \u0026onlyfilesFS{fs} } 这里主要是能够支持是否列出目录下的内容（HTTP 默认文件的实现） 静态目录是关闭的，文件服务器也可以配置一样的效果 func (f neuteredReaddirFile) Readdir(count int) ([]os.FileInfo, error) { // this disables directory listing return nil, nil } 实现返回 nil，和 Dir(root, false) 配置的 false 关联 2.1.3 文件服务器 能够通过 HTTP 请求访问文件，类似于 FTP 服务器。 使用： r.StaticFS(\"/more_static\", http.Dir(dir+\"/src/gin/assets\")) 效果如下： 文件服务器 源码分析： // StaticFS works just like `Static()` but a custom `http.FileSystem` can be used instead. // Gin by default user: gin.Dir() func (group *RouterGroup) StaticFS(relativePath string, fs http.FileSystem) IRoutes { if strings.Contains(relativePath, \":\") || strings.Contains(relativePath, \"*\") { panic(\"URL parameters can not be used when serving a static folder\") } handler := group.createStaticHandler(relativePath, fs) urlPattern := path.Join(relativePath, \"/*filepath\") // Register GET and HEAD handlers group.GET(urlPattern, handler) group.HEAD(urlPattern, handler) return group.returnObj() } GET 和 HEAD 请求都会到 StaticHandler，所以看下面代码如何创建这个 Handler 即可 /github.com/gin-gonic/gin/routergroup.go:185 func (group *RouterGroup) createStaticHandler(relativeP","date":"2020-10-23","objectID":"/2020/10/gin-static/:2:1","tags":["静态资源","React","Ant Design"],"title":"一拳搞定 Gin | 前后端分离下静态资源部署到后端访问","uri":"/2020/10/gin-static/"},{"categories":["Gin"],"content":"2.2 单页应用实战 上面讲的都是 gin 本身基础内容，下面说下项目中的实战例子。 2.2.1 静态文件位置 项目文件夹如下： 目录 dist 前端 npm intall 打出来的包 pkg 后端代码 dist目录 index.html 单页应用，只有一个 HTML，其它都是 .js 渲染出来的 2.2.2 项目配置 通过下面代码加到 gin 里面。 root := path.Join(dir, \"dist\") r.Use(static.NewFileSystem(root, \"/\", constant.ApiVersion, false).HandlerFunc()) 参数说明： root 文件服务的根目录 \"/\" 规则，既匹配全部 不包含的路径，指后端接口路径，比如 /api/v1 false 上面提到默认的实现有体现，ture 会罗列文件夹下的文件列表，用作 FTP 服务器 2.2.3 原理说明 自定义的 fileSystem type FileSystem struct { root string prefix string exclude string indexes bool http.FileSystem http.Handler } root 根路径，文件系统用 prefix 请求前缀，按我的场景是 \"/\"，如果你有类似 /health 的这种接口，那么请扩展 exclude 的逻辑 exclude 忽略的路径，我的场景是 /api/v1，所以的后端接口默认都带上这个路径 indexes 是否遍历子目录查找，我的场景是不需要的，所以配置 false gin 对请求的处理逻辑 func (fs *FileSystem) HandlerFunc() gin.HandlerFunc { return func(c *gin.Context) { if fs.exclude != \"\" \u0026\u0026 strings.HasPrefix(c.Request.URL.Path, fs.exclude) { c.Next() } else { if fs.Exist(c) { fs.ServeHTTP(c.Writer, c.Request) c.Abort() } } } } 如果是 /api/v1 后端接口，则直接进入下一步 否则判断是否存在，存在会执行标准的 ServeHTTP 判断文件是否存在 func (fs *FileSystem) Exist(c *gin.Context) bool { filepath := c.Request.URL.Path if isFrontPageRouter(filepath, fs.exclude) { return true } if p := strings.TrimPrefix(filepath, fs.prefix); len(p) \u003c len(filepath) { name := path.Join(fs.root, p) stats, err := os.Stat(name) if err != nil { return false } if stats.IsDir() { if !fs.indexes { index := path.Join(name, INDEX) _, err := os.Stat(index) if err != nil { return false } } } return true } return false } 如果是前端页面，直接返回存在，因为始终是 index.html 其它用 os 去判断文件是否真实存在 http.HandlerFunc 改进 // StripPrefix like http.StripPrefix func StripPrefix(prefix, exclude string, h http.Handler) http.Handler { if prefix == \"\" { return h } return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { if p := strings.TrimPrefix(r.URL.Path, prefix); len(p) \u003c len(r.URL.Path) { r2 := new(http.Request) *r2 = *r r2.URL = new(url.URL) *r2.URL = *r.URL r2.URL.Path = p b := isFrontPageRouter(r.URL.Path, exclude) if b { r2.URL.Path = \"/\" } h.ServeHTTP(w, r2) } else { http.NotFound(w, r) } }) } 新加参数 exclude，表示是否不包含的路径 isFrontPageRouter 是否是前端页面的路径，如果是前端页面会改写 URL.Path = \"/\"，保证始终访问 index.html 文件 ","date":"2020-10-23","objectID":"/2020/10/gin-static/:2:2","tags":["静态资源","React","Ant Design"],"title":"一拳搞定 Gin | 前后端分离下静态资源部署到后端访问","uri":"/2020/10/gin-static/"},{"categories":["Gin"],"content":"2.3 扩展阅读 2.3.1 前端页面 扩展官方的示例，如何返回一个 HTML func main() { r := gin.Default() // Serves unicode entities r.GET(\"/json\", func(c *gin.Context) { c.JSON(200, gin.H{ \"html\": \"\u003cb\u003eHello, world!\u003c/b\u003e\", }) }) // Serves literal characters r.GET(\"/purejson\", func(c *gin.Context) { c.PureJSON(200, gin.H{ \"html\": \"\u003cb\u003eHello, world!\u003c/b\u003e\", }) }) // listen and serve on 0.0.0.0:8080 r.Run(\":8080\") } 上面的两个请求返回效果如图： json pure ","date":"2020-10-23","objectID":"/2020/10/gin-static/:2:3","tags":["静态资源","React","Ant Design"],"title":"一拳搞定 Gin | 前后端分离下静态资源部署到后端访问","uri":"/2020/10/gin-static/"},{"categories":["旅游"],"content":"2020-10-17 ~ 2010-10-18 黄山二日游记。 ","date":"2020-10-19","objectID":"/2020/10/tour-china-huangshan/:0:0","tags":["黄山","短途游"],"title":"黄山2日1晚游玩","uri":"/2020/10/tour-china-huangshan/"},{"categories":["旅游"],"content":"一、前言 小时候就知道一句话“五岳归来不看山，黄山归来不看岳”。所以心里一直对黄山碎碎念，前些天定了黄山上的白云宾馆（不能退），终于能圆我的黄山梦了。 这次去黄山是和老婆两个人的周末休闲游，墨迹天气17，18两天是阴天到小雨的天气，查看黄山的攻略有说到：阴天就是在雾里穿行，啥都看不到。 我们带着一丝奔溃和无奈开启了这趟黄山之行。 ","date":"2020-10-19","objectID":"/2020/10/tour-china-huangshan/:1:0","tags":["黄山","短途游"],"title":"黄山2日1晚游玩","uri":"/2020/10/tour-china-huangshan/"},{"categories":["旅游"],"content":"二、准备工作 飞猪订的酒店，黄山白云宾馆 B区普通标间，含双早（不吃饱肯定是不行的） 自驾（从家到杭州东及黄山北到黄山风景区，都有一定的距离，坐车耗时） 查看登山路线 地图 在黄山风景区 公共号|小程序 提前买好票 携程制作的行程 day1 9点开车出发，3个小时左右到黄山，吃当地的菜 从云谷寺坐云谷索道的缆车上山，估计2点到山上 后山上，随缘游玩白鹅岭，始信峰，狮子峰，丹霞峰，排云亭，飞来石 入住白云宾馆 day2 早起到光明顶看日出 吃饱早餐后前往前山 随缘游玩天海，百步云梯，迎客松 玉屏索道下山，计划中午12点半左右 如果来得及去宏村逛逛 开车回杭州 准备物品 一次性雨衣，登山杖，手套 身份证，充电器，扎实的双肩背包 牛肉干，巧克力，薯片 ","date":"2020-10-19","objectID":"/2020/10/tour-china-huangshan/:2:0","tags":["黄山","短途游"],"title":"黄山2日1晚游玩","uri":"/2020/10/tour-china-huangshan/"},{"categories":["旅游"],"content":"三、过程 ","date":"2020-10-19","objectID":"/2020/10/tour-china-huangshan/:3:0","tags":["黄山","短途游"],"title":"黄山2日1晚游玩","uri":"/2020/10/tour-china-huangshan/"},{"categories":["旅游"],"content":"day1 6点的时候就起床了，确实是每次出门前总是容易醒得早。加满油，吃饱饭，不紧不慢的收拾行李，把厚衣服都穿在了身上，（其它也就一个书包的东西，多余的都放车上就行了）。 周六虽然天气预报是阴天，但是杭州是阳关明媚，路上车也不多，舒舒服服的开车到老余杭的杭瑞高速入口前往黄山。 出发路线 255公里左右 首先路过未来科技城南湖、临安青山湖，随着未来科技城的发展，高楼林立，浓浓的城市化痕迹。随后进去临安区，虽然城市化发展也很明显，但是四周高山围绕，已经慢慢从都市到了大山的感觉。等到行至昌化镇大明山附近，能看到路边的一些村落，剩下的都是连绵的山峰，高速路在山中穿行，大自然的广阔景象让人心神宁静。当路过一个隧道一瞬间就进入安徽境内，路标有新安江山水画廊，那一段能看到一点点的新安江，眼前的村落都变成徽派建筑，白墙黑瓦。不过天已经变阴天了，都是厚厚的云层，过了歙县后就进去黄山市的徽州区了，高速走南北向的京台高速，一路开到汤口出高速就在黄山南大门附近了，我下去就随手找了个停车场，后来才知道南大门也有停车场，可以少走一段路。 停好车肚子已经咕咕叫了，找了家餐馆：老谢徽菜馆，点了大众点评推荐菜前二名：灵芝草和铁板臭鳜鱼。灵芝草口味满不错的，但是铁板臭鳜鱼感觉很一般，没臭豆腐好吃，也没家里做的鱼好吃，结合到一起口感怪怪的。另外一点就是黄山吃的偏贵，点了两个菜加两个米饭，一共176块钱，不过可以使用50减40的券，相当于是146。 步行前往南大门坐车，路过肯德基，又买了些作为爬山中途的能量补充。强烈建议去黄山可以多买点肯德基，实惠又好吃，补充能量的不二选择。 南大门进去一路刷身份证就行，不过需要坐大巴到云谷寺（差不多花了半个小时），然后在云谷寺坐云谷索道上山，大概12分钟。我是2点20分坐上的缆车，因为山上温度较低，已经步入秋季临近冬季，能看到黄山在绿色的背景下有一些黄色和红色的点缀，缆车外面灰蒙蒙的导致手机无法拍出效果。渐渐就看到山里大雾缭绕，索道上升到后面的时候基本上完全看不到四周，只感觉在云雾中串行，特别刺激。 即将进入大雾中的缆车 上山后，先向始信峰方向走。雾一开始感觉还在远处，近处的山峰还是清晰可见： 远处的雾1 有些地方雾不大，可以看到的山景多点： 远处的雾2 到三点半之后，山中的雾越来越大，下图是始信峰附近的松树： 松树 可能几分钟前还能看到，但是拍两张照片的功夫，已经快看不清对面的山峰了。 始信峰附近 四点半的时候来到北海宾馆（第一个大分叉，向北可以去看猴子观海，狮子峰），那时候天色已经渐渐暗下来，也开始冷起来了，因为肯定看不到晚霞，于是就向西光明顶方向走。 阴天加大雾，景色基本上也没啥可以看的，赶了一个小时的山路，到五点半时候，空气中的湿度已经很大了，已经是雨天的感觉了。 湿雾 悲惨时间 天色开始变暗，此时内心就想着早点赶回酒店了。山上天暗下来后冷暗交加，有点流落在外的凄凉感。离光明顶100米左右的时候，天开始下雨，刚穿好一次性雨衣，发现居然是冰雹，虽然颗粒不大，密密麻麻砸在手上还是有点疼。到了光明顶后，有四个岔路，天已经黑了，我有点像无头苍蝇，走到光明顶山庄才发现是断头路，白云宾馆还要往下走，那时候很绝望，下次再去黄山酒店必须定在光明顶山庄，早点上山，在后山逛一圈到光明顶山庄休息刚刚好傍晚，可以看晚霞，也方便休息。 穿着雨衣乌漆嘛黑的走了10分钟左右的下山路才看到白云宾馆（更加绝望的是看日出还要爬上去。。。）。 另外酒店自助餐160一位，对面餐馆没有套餐，点菜一两百一个菜，蛋炒饭88，最后我们买了八宝粥，冰糖雪梨，加上山下买的KFC也是可以将就的。 幸福时间 终于到了酒店，房间虽然比较小，也比较简陋，但是床，卫生间都很干净，还有暖气，进了房间躺床上舒服的不得了。不过刚刚经历了冰雹，我对明天的日出基本上不抱有任何希望了，想着好好睡一觉，明天更加随缘了。 酒店方给出了第二天观景的概率，日出45%，云海55%，数据还是让人重新有了希望，看来早上起来能看到点东西。 ","date":"2020-10-19","objectID":"/2020/10/tour-china-huangshan/:3:1","tags":["黄山","短途游"],"title":"黄山2日1晚游玩","uri":"/2020/10/tour-china-huangshan/"},{"categories":["旅游"],"content":"day2 5点10分的闹钟准时起来，窗外还是黑夜，我第一时间看了黄山旅游的小程序，光明顶的天气显示是有太阳的，突然就有了活力，等老婆洗漱好，我们穿上羽绒服开启爬山模式。刚爬了没几步，日出的方向就看到有一点点霞光漏出来，很让人激动。 酒店 下面是日出的图： 即将日出 日出 日出火烧云 日出真的是非常的美，黄山日出果真是最值得期待的。 下面是云海的图： 云海1 云海2 云海3 云海4 光明顶阳光明媚的清晨： 阳光明媚 回宾馆的路上，两边都是松树，特别的黄山味： 松树路 看完日出，回宾馆吃早饭，然后就去前山准备下山了，前山比后山难走，当得知去百步云梯和莲花峰起码一个小时就放弃了。选择了最基本路线从鳌鱼峰走到引客松。 迎客松 下山的时候一点了，刚到南大门就开始下雨，下雨天宏村也看不到什么景色，人也累了，就早点回杭州了。 ","date":"2020-10-19","objectID":"/2020/10/tour-china-huangshan/:3:2","tags":["黄山","短途游"],"title":"黄山2日1晚游玩","uri":"/2020/10/tour-china-huangshan/"},{"categories":["旅游"],"content":"费用 费用项 花费 门票 796 车费 445 住宿 1240 吃饭 284 其它 73 合计 2848 人均花费 1424 元 ","date":"2020-10-19","objectID":"/2020/10/tour-china-huangshan/:4:0","tags":["黄山","短途游"],"title":"黄山2日1晚游玩","uri":"/2020/10/tour-china-huangshan/"},{"categories":["旅游"],"content":"总结 黄山的景色很美，特别像山水画，仿佛人入画境。 黄山建议玩2天，第一天早点上山，上午的天气比较好。 黄山看日出更建议住光明顶，看日出、日落和云海都很方便。 黄山的当地菜很一般，爬山还是买KFC更有力。 ","date":"2020-10-19","objectID":"/2020/10/tour-china-huangshan/:5:0","tags":["黄山","短途游"],"title":"黄山2日1晚游玩","uri":"/2020/10/tour-china-huangshan/"},{"categories":["Node.js"],"content":"本文介绍如何在本地启动 nodejs 项目。 ","date":"2020-10-11","objectID":"/2020/10/nodejs-start/:0:0","tags":["入门","前端"],"title":"一拳搞定 Node.js | 如何运行一个 node 项目","uri":"/2020/10/nodejs-start/"},{"categories":["Node.js"],"content":"一、前言 最近要搭建自动化运维平台，前后端的生杀大权都随自己定，前端的技术选型了蚂蚁的 ant design pro，于是乎把折腾过的 node 和 react 的相关知识记录下。 注意： 本文需要读者具备基础的编程能力以及对计算机网络的基本了解，一些常用术语限于篇幅不再展开。 本文基于 mac 系统开发，如果 win 下有不一致的情况请自行对照相关命令。 ","date":"2020-10-11","objectID":"/2020/10/nodejs-start/:1:0","tags":["入门","前端"],"title":"一拳搞定 Node.js | 如何运行一个 node 项目","uri":"/2020/10/nodejs-start/"},{"categories":["Node.js"],"content":"二、准备环境 ","date":"2020-10-11","objectID":"/2020/10/nodejs-start/:2:0","tags":["入门","前端"],"title":"一拳搞定 Node.js | 如何运行一个 node 项目","uri":"/2020/10/nodejs-start/"},{"categories":["Node.js"],"content":"2.1、安装 Node.js Node.js 发布版本分为 Current 和 LTS 两类，前者每 6 个月迭代一个大版本，快速提供新增功能和问题修复，后者以 30 个月为一个大周期，为生产环境提供稳定依赖。当前 Node.js 最新 LTS 版本为 12.18.2，本文以此版本作为运行环境，可以在官网下载并安装。 安装完成后，在命令行输入 node --version 查看输出是否为 v12.8.2，如果一致，那么安装成功。 另外，有兴趣的读者可以尝试通过 nvm / nvm-windows 管理多个 Node.js 版本，此处不是本文重点不再展开。 我本机的版本： $ node -v v12.18.2 ","date":"2020-10-11","objectID":"/2020/10/nodejs-start/:2:1","tags":["入门","前端"],"title":"一拳搞定 Node.js | 如何运行一个 node 项目","uri":"/2020/10/nodejs-start/"},{"categories":["Node.js"],"content":"三、创建 node 项目 ","date":"2020-10-11","objectID":"/2020/10/nodejs-start/:3:0","tags":["入门","前端"],"title":"一拳搞定 Node.js | 如何运行一个 node 项目","uri":"/2020/10/nodejs-start/"},{"categories":["Node.js"],"content":"3.1 初始化工程 3.1.1 创建并进入项目目录 mkdir node-sample \u0026\u0026 cd node-sample 3.1.2 初始化 package.json $ npm init This utility will walk you through creating a package.json file. It only covers the most common items, and tries to guess sensible defaults. See `npm help init` for definitive documentation on these fields and exactly what they do. Use `npm install \u003cpkg\u003e` afterwards to install a package and save it as a dependency in the package.json file. Press ^C at any time to quit. package name: (sample) demo version: (1.0.0) 0.1.0 description: demo node entry point: (index.js) test command: git repository: keywords: author: license: (ISC) About to write to /Users/tc/Documents/workspace_2020/node-sample/package.json: { \"name\": \"demo\", \"version\": \"0.1.0\", \"description\": \"demo node\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"author\": \"\", \"license\": \"ISC\" } Is this OK? (yes) yes 查看生成的文件或目录 $ ls node_modules package-lock.json package.json 3.1.3 使用 express 安装 express npm install express 创建目录和文件 mkdir src touch src/server.js 编写 server.js const express = require('express') const app = express() const port = 3000 app.get('/', (req, res) =\u003e { res.send('Hello World!') }) app.listen(port, () =\u003e { console.log(`Example app listening at http://localhost:${port}`) }) 3.1.4 启动服务 启动 $ node src/server.js Example app listening at http://localhost:3000 请求 请求 停止服务 control + c 端口是否在使用 $ lsof -i:3000 ","date":"2020-10-11","objectID":"/2020/10/nodejs-start/:3:1","tags":["入门","前端"],"title":"一拳搞定 Node.js | 如何运行一个 node 项目","uri":"/2020/10/nodejs-start/"},{"categories":["Node.js"],"content":"3.2、静态资源服务器 3.2.1 创建目录和文件 创建 public 文件夹 mkdir src 创建 HTML 文件 touch src/index.html 查看目录 $ tree -L 1 . ├── index.js ├── node_modules ├── package-lock.json ├── package.json ├── public └── src 3.2.2 编写 index.html \u003c!doctype html\u003e \u003chtml\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\" /\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eIt works!\u003c/h1\u003e \u003c/body\u003e \u003c/html\u003e 3.2.3 编写 server.js const express = require('express') const { resolve } = require('path'); const { promisify } = require('util'); const app = express() const port = parseInt(process.env.PORT || '3000'); const publicDir = resolve('public'); async function bootstrap() { app.use(express.static(publicDir)) await promisify(app.listen.bind(app, port))(); console.log(`\u003e Started on port http://localhost:${port}`); } bootstrap(); 3.2.4 启动服务 启动 npm start \u003e demo@0.1.0 start /Users/tc/Documents/workspace_2020/node-sample \u003e node src/server.js \u003e Started on port http://localhost:3000 请求 请求 ","date":"2020-10-11","objectID":"/2020/10/nodejs-start/:3:2","tags":["入门","前端"],"title":"一拳搞定 Node.js | 如何运行一个 node 项目","uri":"/2020/10/nodejs-start/"},{"categories":["Node.js"],"content":"四、碰到问题 ","date":"2020-10-11","objectID":"/2020/10/nodejs-start/:4:0","tags":["入门","前端"],"title":"一拳搞定 Node.js | 如何运行一个 node 项目","uri":"/2020/10/nodejs-start/"},{"categories":["Node.js"],"content":"4.1 端口被占用 配置好 package.json 的 start script 后，启动项目。 npm start \u003e demo@0.1.0 start /Users/tc/Documents/workspace_2020/node-sample \u003e node src/server.js events.js:292 throw er; // Unhandled 'error' event ^ Error: listen EADDRINUSE: address already in use :::3000 at Server.setupListenHandle [as _listen2] (net.js:1313:16) at listenInCluster (net.js:1361:12) at Server.listen (net.js:1447:7) at Function.listen (/Users/tc/Documents/workspace_2020/node-sample/node_modules/express/lib/application.js:618:24) at internal/util.js:297:30 at new Promise (\u003canonymous\u003e) at bound listen (internal/util.js:296:12) at bootstrap (/Users/tc/Documents/workspace_2020/node-sample/src/server.js:15:46) at Object.\u003canonymous\u003e (/Users/tc/Documents/workspace_2020/node-sample/src/server.js:19:1) at Module._compile (internal/modules/cjs/loader.js:1138:30) Emitted 'error' event on Server instance at: at emitErrorNT (net.js:1340:8) at processTicksAndRejections (internal/process/task_queues.js:84:21) { code: 'EADDRINUSE', errno: 'EADDRINUSE', syscall: 'listen', address: '::', port: 3000 } npm ERR! code ELIFECYCLE npm ERR! errno 1 npm ERR! demo@0.1.0 start: `node src/server.js` npm ERR! Exit status 1 npm ERR! npm ERR! Failed at the demo@0.1.0 start script. npm ERR! This is probably not a problem with npm. There is likely additional logging output above. npm ERR! A complete log of this run can be found in: npm ERR! /Users/tc/.npm/_logs/2020-11-10T02_13_36_826Z-debug.log 可以看到错误日志的日志文件位置和错误信息，地址已经被占用。 ","date":"2020-10-11","objectID":"/2020/10/nodejs-start/:4:1","tags":["入门","前端"],"title":"一拳搞定 Node.js | 如何运行一个 node 项目","uri":"/2020/10/nodejs-start/"},{"categories":["Node.js"],"content":"五、参考 https://expressjs.com/en/starter/hello-world.html https://segmentfault.com/a/1190000023311123 ","date":"2020-10-11","objectID":"/2020/10/nodejs-start/:5:0","tags":["入门","前端"],"title":"一拳搞定 Node.js | 如何运行一个 node 项目","uri":"/2020/10/nodejs-start/"},{"categories":["kubernetes"],"content":"本文介绍通过 Kubernetes Deployment 对象如何去控制一个应用。 ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:0:0","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"一、前言 学习目标: 创建一个 nginx deployment。 使用 kubectl 对 deployment 进行 CRUD。（结合 dashboard 图示） 使用 go 客户端进行上述操作。 默认已经安装了 kubernetes。如果没安装，请参考搭建 minikube ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:1:0","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"二、实战操作 ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:2:0","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"2.1 准备 yaml 文件 按照任务创建一个运行 nginx:1.14.2 Docker 镜像的 Deployment: apiVersion:apps/v1# for versions before 1.9.0 use apps/v1beta2kind:Deploymentmetadata:name:nginx-deploymentnamespace:testspec:selector:matchLabels:app:nginxreplicas:2# tells deployment to run 2 pods matching the templatetemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.14.2ports:- containerPort:80文件说明： 在命名空间 test（由 .metadata.namespace 字段标明）创建名为 nginx-deployment（由 .metadata.name 字段标明）的 Deployment。 该 Deployment 创建两个（由 .spec.replicas 字段标明）Pod 副本。 selector 字段定义 Deployment 如何查找要管理的 Pods。 在这里，你只需选择在 Pod 模板中定义的标签（app: nginx）。 不过，更复杂的选择规则是也可能的，只要 Pod 模板本身满足所给规则即可。 说明 matchLabels 字段是 {key,value} 偶对的映射。在 matchLabels 映射中的单个 {key,value} 映射等效于 matchExpressions 中的一个元素，即其 key 字段是 “key”，operator 为 “In”，value 数组仅包含 “value”。在 matchLabels 和 matchExpressions 中给出的所有条件都必须满足才能匹配。 template 字段包含以下子字段： Pod 被使用 labels 字段打上 app: nginx 标签。 Pod 模板规约（即 .template.spec 字段）指示 Pods 运行一个 nginx 容器， 该容器运行版本为 1.14.2 的 nginx Docker Hub镜像。 创建一个容器并使用 name 字段将其命名为 nginx。 ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:2:1","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"2.2 创建 Deployment 1. 运行 yaml 文件创建 也可以通过命令直接创建，参考minikube kubectl apply -f /XXX/document/note/Kubernetes/test/nginx/deployment.yaml 小贴士 说明： 你可以设置 –record 标志将所执行的命令写入资源注解 kubernetes.io/change-cause 中。 这对于以后的检查是有用的。例如，要查看针对每个 Deployment 修订版本所执行过的命令。 2. 查看 deployment 创建情况 kubectl get deployments --namespace=test 输出： NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 2/2 2 2 8d 输出字段说明： NAME 列出了集群中 Deployment 的名称。 READY 显示应用程序的可用的 副本 数。显示的模式是“就绪个数/期望个数”。 UP-TO-DATE 显示为了达到期望状态已经更新的副本数。 AVAILABLE 显示应用可供用户使用的副本数。 AGE 显示应用程序运行的时间。 3. 查看 Deployment 上线状态： kubectl rollout status deployment.v1.apps/nginx-deployment --namespace=test 输出： deployment \"nginx-deployment\" successfully rolled out 4. 查看 Deployment 创建的 ReplicaSet（rs）： kubectl get rs --namespace=test 输出： NAME DESIRED CURRENT READY AGE nginx-deployment-6b5df77cd8 2 2 2 17h nginx-deployment-74f5bf7bd9 0 0 0 8d nginx-deployment-7c79566d49 0 0 0 17h nginx-deployment-865d6c94c9 0 0 0 17h nginx-deployment-fbb99c8b8 0 0 0 17h 输出字段说明： NAME 列出名字空间中 ReplicaSet 的名称； DESIRED 显示应用的期望副本个数，即在创建 Deployment 时所定义的值。 此为期望状态； CURRENT 显示当前运行状态中的副本个数； READY 显示应用中有多少副本可以为用户提供服务； AGE 显示应用已经运行的时间长度。 5. 展示 Deployment 的更多信息： kubectl describe deployment nginx-deployment --namespace=test 输出： Name: nginx-deployment Namespace: test CreationTimestamp: Thu, 01 Oct 2020 17:12:59 +0800 Labels: \u003cnone\u003e Annotations: deployment.kubernetes.io/revision: 3 kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"name\":\"nginx-deployment\",\"namespace\":\"test\"},\"spec\":{\"replicas\":... Selector: app=nginx Replicas: 2 desired | 2 updated | 2 total | 2 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.14.2 Port: 80/TCP Host Port: 0/TCP Limits: memory: 200Mi Requests: memory: 100Mi Environment: \u003cnone\u003e Mounts: \u003cnone\u003e Volumes: \u003cnone\u003e Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u003cnone\u003e NewReplicaSet: nginx-deployment-fbb99c8b8 (2/2 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 5m6s deployment-controller Scaled up replica set nginx-deployment-865d6c94c9 to 1 Normal ScalingReplicaSet 3m43s deployment-controller Scaled down replica set nginx-deployment-74f5bf7bd9 to 1 Normal ScalingReplicaSet 3m43s deployment-controller Scaled up replica set nginx-deployment-fbb99c8b8 to 1 Normal ScalingReplicaSet 3m41s deployment-controller Scaled down replica set nginx-deployment-865d6c94c9 to 0 Normal ScalingReplicaSet 3m41s deployment-controller Scaled up replica set nginx-deployment-fbb99c8b8 to 2 Normal ScalingReplicaSet 3m38s deployment-controller Scaled down replica set nginx-deployment-74f5bf7bd9 to 0 输出字段说明： Events rs 根据 deployment 的配置控制 pod 的行为，可以看到当副本数变化时，旧的 rs 缩容到 0，新的 rs 扩容到 2（是从上往下的顺序执行，渐进式，结合下一个小贴士便于理解）。 6. 列出创建的 pods： kubectl get pods -l app=nginx --namespace=test 输出： NAME READY STATUS RESTARTS AGE nginx-deployment-fbb99c8b8-4lpwd 1/1 Running 0 5m28s nginx-deployment-fbb99c8b8-tpd4n 1/1 Running 0 5m26s 小贴士 Deployment 可确保在更新时仅关闭一定数量的 Pod。默认情况下，它确保至少所需 Pods 75% 处于运行状态（最大不可用比例为 25%）。 Deployment 还确保仅所创建 Pod 数量只可能比期望 Pods 数高一点点。 默认情况下，它可确保启动的 Pod 个数比期望个数最多多出 25%（最大峰值 25%）。 7. 查看具体某个 pod 的信息： kubectl describe pod nginx-deployment-fbb99c8b8-4lpwd --namespace=test 输出： Name: nginx-deployment-fbb99c8b8-4lpwd Namespace: test Priority: 0 Node: minikube/192.168.64.3 Start Time: Fri, 09 Oct 2020 15:40:15 +0800 Labels: app=nginx pod-template-hash=fbb99c8b8 Annotations: \u003cnone\u003e Status: Running IP: 172.17.0.9 IPs: IP: 172.17.0.9 Controlled By: ReplicaSet/nginx-deployment-fbb99c8b8 Containers: nginx: Container ID: docker://16da80166234e8f7fdbb2e2bb7accbcc1841213cb2888af6463102675414c00d Image: nginx:1.14.2 Image ID: docker-pullable://nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df","date":"2020-10-09","objectID":"/2020/10/run-deployment/:2:2","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"2.3 更新 deployment 更新镜像，把 nginx 从 1.14.2 升级到 1.19.3 1. 直接修改配置文件 apiVersion:apps/v1# for versions before 1.9.0 use apps/v1beta2kind:Deploymentmetadata:name:nginx-deploymentnamespace:testspec:selector:matchLabels:app:nginxreplicas:2# tells deployment to run 2 pods matching the templatetemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.19.3ports:- containerPort:80再次执行 kubectl apply -f 指令。 2. 执行 kubectl 编辑命令 kubectl --record deployment.apps/nginx-deployment set image \\ deployment.v1.apps/nginx-deployment nginx=nginx:1.19.3 --namespace=test 或者 kubectl set image deployment/nginx-deployment nginx=nginx:1.19.3 --record --namespace=test 或者 kubectl edit deployment.v1.apps/nginx-deployment --namespace=test # 等价 kubectl edit deployment/nginx-deployment --namespace=test 编辑效果： edit-deploymentedit-deployment \" edit-deployment 查看新的 pods: kubectl get pods -l app=nginx --namespace=test NAME READY STATUS RESTARTS AGE nginx-deployment-6b5df77cd8-lf2cl 1/1 Running 0 69s nginx-deployment-6b5df77cd8-n52z6 1/1 Running 0 71s 可以看到和之前的名称都是不一样的。 dashboard可以直观看到: PodsPods \" Pods ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:2:3","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"2.4 删除 deployment 通过名称删除deployment: kubectl delete deployment nginx-deployment --namespace=test ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:2:4","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"2.5 改变副本数来弹性伸缩应用 关键参数 replicas 可以设置 pod 的数量，下面例子把 nginx 变成 4 个。 1. 直接修改配置文件 apiVersion:apps/v1# for versions before 1.9.0 use apps/v1beta2kind:Deploymentmetadata:name:nginx-deploymentnamespace:testspec:selector:matchLabels:app:nginxreplicas:4# tells deployment to run 2 pods matching the templatetemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.19.3ports:- containerPort:80再次执行 kubectl apply -f 指令。 查看新的 pods: kubectl get pods -l app=nginx --namespace=test NAME READY STATUS RESTARTS AGE nginx-deployment-6b5df77cd8-5b28p 1/1 Running 0 65s nginx-deployment-6b5df77cd8-b24xd 1/1 Running 0 65s nginx-deployment-6b5df77cd8-lf2cl 1/1 Running 0 17m nginx-deployment-6b5df77cd8-n52z6 1/1 Running 0 17m 2. 执行 kubectl 编辑命令 kubectl scale deployment/nginx-deployment --replicas=10 --namespace=test 扩展点（另起文章学习）： 假设集群启用了Pod 的水平自动缩放， 你可以为 Deployment 设置自动缩放器，并基于现有 Pods 的 CPU 利用率选择 要运行的 Pods 个数下限和上限。 kubectl autoscale deployment/nginx-deployment --min=10 --max=15 --cpu-percent=80 --namespace=test 3. 比例缩放实战 默认的配置 spec:strategy:type:RollingUpdaterollingUpdate:maxUnavailable:25%maxSurge:25%说明 Deployment 会在 .spec.strategy.type==RollingUpdate时，采取 滚动更新的方式更新 Pods。你可以指定 maxUnavailable 和 maxSurge 来控制滚动更新 过程。 最大不可用 .spec.strategy.rollingUpdate.maxUnavailable 是一个可选字段，用来指定 更新过程中不可用的 Pod 的个数上限。该值可以是绝对数字（例如，5），也可以是 所需 Pods 的百分比（例如，10%）。百分比值会转换成绝对数并去除小数部分。 如果 .spec.strategy.rollingUpdate.maxSurge 为 0，则此值不能为 0。 默认值为 25%。 最大峰值 .spec.strategy.rollingUpdate.maxSurge 是一个可选字段，用来指定可以创建的超出 期望 Pod 个数的 Pod 数量。此值可以是绝对数（例如，5）或所需 Pods 的百分比（例如，10%）。 如果 MaxUnavailable 为 0，则此值不能为 0。百分比值会通过向上取整转换为绝对数。 此字段的默认值为 25%。 更新 Deployment 使用新镜像，碰巧该镜像无法从集群内部解析。 kubectl set image deployment/nginx-deployment nginx=nginx:sometag --namespace=test 输出： deployment.apps/nginx-deployment image updated 镜像更新使用新的 ReplicaSet nginx-deployment-86dc975d96 启动上线过程，但由于默认 maxUnavailable 的要求会被阻塞。检查上线状态： kubectl get rs --namespace=test 输出： NAME DESIRED CURRENT READY AGE nginx-deployment-6b5df77cd8 2 2 2 18h nginx-deployment-86dc975d96 1 1 0 112s dashboard: PodsPods \" Pods 查看 deployment 创建情况 kubectl get deploy --namespace=test 输出： NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 2/2 1 2 8d 变更 Deployment 副本数到 10 kubectl scale deployment/nginx-deployment --replicas=10 --namespace=test 输出： deployment.apps/nginx-deployment scaled 再次查看 deployment 创建情况 kubectl get deploy --namespace=test 输出： NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 8/10 5 8 8d 确认上线状态 kubectl get rs --namespace=test 输出： NAME DESIRED CURRENT READY AGE nginx-deployment-6b5df77cd8 8 8 8 18h nginx-deployment-86dc975d96 5 5 0 6m56s 说明： 10 个 pod，25% 不可用，所以不能超过 3 个，10 - 2 = 8 个可用，25% 的最大值，就是 12.5，向上取整 13， 8 + 5 = 13。 ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:2:5","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"2.6 回滚 deployment 回滚是 CI/CD 中重要的一个环节，在 kubernetes 面向镜像的无状态服务很好的支持了回滚这个操作。在学习这段内容之前我的做法是直接更新镜像到之前的版本。 默认情况下，Deployment 的所有上线记录都保留在系统中，以便可以随时回滚 （你可以通过修改修订历史记录限制来更改这一约束）。 小贴士 Deployment 被触发上线时，系统就会创建 Deployment 的新的修订版本。 这意味着仅当 Deployment 的 Pod 模板（.spec.template）发生更改时，才会创建新修订版本 – 例如，模板的标签或容器镜像发生变化。 其他更新，如 Deployment 的扩缩容操作不会创建 Deployment 修订版本。 这是为了方便同时执行手动缩放或自动缩放。 换言之，当你回滚到较早的修订版本时，只有 Deployment 的 Pod 模板部分会被回滚。 回滚操作基于比例缩放实战 查看 pod kubectl get pod --selector app=nginx --namespace=test 输出： NAME READY STATUS RESTARTS AGE nginx-deployment-6b5df77cd8-6wx22 1/1 Running 0 24m nginx-deployment-6b5df77cd8-85q77 1/1 Running 0 24m nginx-deployment-6b5df77cd8-8jstb 1/1 Running 0 24m nginx-deployment-6b5df77cd8-lf2cl 1/1 Running 0 19h nginx-deployment-6b5df77cd8-lg2kz 1/1 Running 0 24m nginx-deployment-6b5df77cd8-n52z6 1/1 Running 0 19h nginx-deployment-6b5df77cd8-nxcrf 1/1 Running 0 24m nginx-deployment-6b5df77cd8-tt44h 1/1 Running 0 24m nginx-deployment-86dc975d96-d9pw7 0/1 ImagePullBackOff 0 24m nginx-deployment-86dc975d96-fx9nr 0/1 ImagePullBackOff 0 24m nginx-deployment-86dc975d96-lfhl4 0/1 ImagePullBackOff 0 24m nginx-deployment-86dc975d96-m252z 0/1 ImagePullBackOff 0 24m nginx-deployment-86dc975d96-p7zcg 0/1 ImagePullBackOff 0 31m 小贴士 Deployment 控制器自动停止有问题的上线过程，并停止对新的 ReplicaSet 扩容。 这行为取决于所指定的 rollingUpdate 参数（具体为 maxUnavailable）。 默认情况下，Kubernetes 将此值设置为 25%。 获取 Deployment 描述信息 kubectl describe deployment nginx-deployment --namespace=test 输出： Name: nginx-deployment Namespace: test CreationTimestamp: Thu, 01 Oct 2020 17:12:59 +0800 Labels: \u003cnone\u003e Annotations: deployment.kubernetes.io/revision: 8 kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"name\":\"nginx-deployment\",\"namespace\":\"test\"},\"spec\":{\"replicas\":... Selector: app=nginx Replicas: 10 desired | 5 updated | 13 total | 8 available | 5 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:sometag Port: 80/TCP Host Port: 0/TCP Limits: memory: 200Mi Requests: memory: 100Mi Environment: \u003cnone\u003e Mounts: \u003cnone\u003e Volumes: \u003cnone\u003e Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing False ProgressDeadlineExceeded OldReplicaSets: nginx-deployment-6b5df77cd8 (8/8 replicas created) NewReplicaSet: nginx-deployment-86dc975d96 (5/5 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 34m deployment-controller Scaled up replica set nginx-deployment-86dc975d96 to 1 Normal ScalingReplicaSet 27m deployment-controller Scaled up replica set nginx-deployment-6b5df77cd8 to 9 Normal ScalingReplicaSet 27m deployment-controller Scaled up replica set nginx-deployment-86dc975d96 to 4 Normal ScalingReplicaSet 27m deployment-controller Scaled down replica set nginx-deployment-6b5df77cd8 to 8 Normal ScalingReplicaSet 27m deployment-controller Scaled up replica set nginx-deployment-86dc975d96 to 5 2.6.1 检查 deployment 上线历史 检查 Deployment 修订历史 kubectl rollout history deployment/nginx-deployment --namespace=test 输出： deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 1 \u003cnone\u003e 2 \u003cnone\u003e 3 \u003cnone\u003e 6 \u003cnone\u003e 7 \u003cnone\u003e 8 \u003cnone\u003e CHANGE-CAUSE 的内容是从 Deployment 的 kubernetes.io/change-cause 注解复制过来的。 复制动作发生在修订版本创建时。你可以通过以下方式设置 CHANGE-CAUSE 消息： 使用 kubectl annotate deployment.v1.apps/nginx-deployment kubernetes.io/change-cause=“image updated to 1.9.1” 为 Deployment 添加注解。 追加 –record 命令行标志以保存正在更改资源的 kubectl 命令。 手动编辑资源的清单。 查看修订历史的详细信息 kubectl rollout history deployment/nginx-deployment --revision=8 --namespace=test deployment.apps/nginx-deployment with revision #8 Pod Template: Labels: app=nginx pod-template-hash=86dc975d96 Containers: nginx: Image: nginx:sometag Port: 80/TCP Host Port: 0/TCP Limits: memory: 200Mi Requests: memory: 100Mi Environment: \u003cnone\u003e Mounts: \u003cnone\u003e Volumes: \u003cnone\u003e 2.6.2 回滚到之前的修订版本 假定现在你已决定撤消当前上线并回滚到以前的修订版本 kub","date":"2020-10-09","objectID":"/2020/10/run-deployment/:2:6","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"三、API操作 client-go 内容也比较多，这里尽量简单介绍。 ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:3:0","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"3.1 创建 deployment 运行 client-go 示例 func TestCreateDeployment(t *testing.T) { c := getClient() n := \"test\" deployment := \u0026appsv1.Deployment{ ObjectMeta: metav1.ObjectMeta{ Name: \"nginx-deployment\", Namespace: n, }, Spec: appsv1.DeploymentSpec{ Replicas: int32Ptr(4), Selector: \u0026metav1.LabelSelector{ MatchLabels: map[string]string{ \"app\": \"nginx\", }, }, Template: apiv1.PodTemplateSpec{ ObjectMeta: metav1.ObjectMeta{ Labels: map[string]string{ \"app\": \"nginx\", }, }, Spec: apiv1.PodSpec{ Containers: []apiv1.Container{ { Name: \"nginx\", Image: \"nginx:1.19.3\", Ports: []apiv1.ContainerPort{ { ContainerPort: 80, }, }, }, }, }, }, }, } result, err := c.AppsV1().Deployments(n).Create(context.Background(), deployment, metav1.CreateOptions{}) assert.NoError(t, err) t.Logf(\"create deployment result : %v\", result) } 查看部署情况 查看 deployment kubectl get deployments --namespace=test 输出： NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 4/4 4 4 3m4s 查看 pod kubectl get pods -l app=nginx --namespace=test 输出： NAME READY STATUS RESTARTS AGE nginx-deployment-669d59f9b4-7lgz2 1/1 Running 0 5m29s nginx-deployment-669d59f9b4-jmj25 1/1 Running 0 5m29s nginx-deployment-669d59f9b4-kswqx 1/1 Running 0 5m29s nginx-deployment-669d59f9b4-sb5xm 1/1 Running 0 5m29s 查看 rs kubectl get rs --namespace=test 输出： NAME DESIRED CURRENT READY AGE nginx-deployment-669d59f9b4 4 4 4 5m18s ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:3:1","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"3.3 查看 deployment 运行 client-go 示例 func TestGetDeployment(t *testing.T) { c := getClient() n := \"test\" result, err := c.AppsV1().Deployments(n).Get(context.TODO(), \"nginx-deployment\", metav1.GetOptions{}) assert.NoError(t, err) t.Logf(\"get deployment result : %v\", result) } ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:3:2","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"3.2 更新 deployment 运行 client-go 示例 func TestUpdateDeployment(t *testing.T) { c := getClient() n := \"test\" retryErr := retry.RetryOnConflict(retry.DefaultRetry, func() error { // Retrieve the latest version of Deployment before attempting update // RetryOnConflict uses exponential backoff to avoid exhausting the apiserver result, getErr := c.AppsV1().Deployments(n).Get(context.TODO(), \"nginx-deployment\", metav1.GetOptions{}) if getErr != nil { t.Errorf(\"Failed to get latest version of Deployment: %v\", getErr) } result.Spec.Replicas = int32Ptr(2) // reduce replica count result.Spec.Template.Spec.Containers[0].Image = \"nginx:1.14.2\" // change nginx version _, updateErr := c.AppsV1().Deployments(n).Update(context.TODO(), result, metav1.UpdateOptions{}) return updateErr }) if retryErr != nil { t.Errorf(\"Update failed: %v\", retryErr) } } 注意下 RetryOnConflict 说明： // RetryOnConflict is used to make an update to a resource when you have to worry about // conflicts caused by other code making unrelated updates to the resource at the same // time. fn should fetch the resource to be modified, make appropriate changes to it, try // to update it, and return (unmodified) the error from the update function. On a // successful update, RetryOnConflict will return nil. If the update function returns a // \"Conflict\" error, RetryOnConflict will wait some amount of time as described by // backoff, and then try again. On a non-\"Conflict\" error, or if it retries too many times // and gives up, RetryOnConflict will return an error to the caller. 一句话总结就是：当你担心有冲突的时候使用，保障出现冲突错误会自动重试一定的次数直到成功。 查看部署情况 查看 deployment kubectl get deployments --namespace=test 输出： NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 2/2 2 2 25m 查看 pod kubectl get pods -l app=nginx --namespace=test 输出： NAME READY STATUS RESTARTS AGE nginx-deployment-574b87c764-2vlqk 1/1 Running 0 5m4s nginx-deployment-574b87c764-5p9br 1/1 Running 0 5m7s 查看 rs kubectl get rs --namespace=test 输出： NAME DESIRED CURRENT READY AGE nginx-deployment-574b87c764 2 2 2 5m20s nginx-deployment-669d59f9b4 0 0 0 25m ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:3:3","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"3.4 删除 deployment 运行 client-go 示例 func TestDeleteDeployment(t *testing.T) { c := getClient() n := \"test\" deletePolicy := metav1.DeletePropagationForeground if err := c.AppsV1().Deployments(n).Delete(context.TODO(), \"nginx-deployment\", metav1.DeleteOptions{ PropagationPolicy: \u0026deletePolicy, }); err != nil { t.Error(err) } } ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:3:4","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"四、补充信息 ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:4:0","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"4.1 Deployment 动态更新 Deployment 控制器每次注意到新的 Deployment 时，都会创建一个 ReplicaSet 以启动所需的 Pods。 如果更新了 Deployment，则控制标签匹配 .spec.selector 但模板不匹配 .spec.template 的 Pods 的现有 ReplicaSet 被缩容。最终，新的 ReplicaSet 缩放为 .spec.replicas 个副本， 所有旧 ReplicaSets 缩放为 0 个副本。 当 Deployment 正在上线时被更新，Deployment 会针对更新创建一个新的 ReplicaSet 并开始对其扩容，之前正在被扩容的 ReplicaSet 会被翻转，添加到旧 ReplicaSets 列表 并开始缩容。 例如，假定你在创建一个 Deployment 以生成 nginx:1.14.2 的 5 个副本，但接下来 更新 Deployment 以创建 5 个 nginx:1.16.1 的副本，而此时只有 3 个nginx:1.14.2 副本已创建。在这种情况下，Deployment 会立即开始杀死 3 个 nginx:1.14.2 Pods， 并开始创建 nginx:1.16.1 Pods。它不会等待 nginx:1.14.2 的 5 个副本都创建完成 后才开始执行变更动作。 ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:4:1","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"4.2 更改标签选择 通常不鼓励更新标签选择算符。建议你提前规划选择算符。 在任何情况下，如果需要更新标签选择算符，请格外小心，并确保自己了解 这背后可能发生的所有事情。 小贴士 在 API 版本 apps/v1 中，Deployment 标签选择算符在创建后是不可变的。 添加选择算符时要求使用新标签更新 Deployment 规约中的 Pod 模板标签，否则将返回验证错误。 此更改是非重叠的，也就是说新的选择算符不会选择使用旧选择算符所创建的 ReplicaSet 和 Pod， 这会导致创建新的 ReplicaSet 时所有旧 ReplicaSet 都会被孤立。 选择算符的更新如果更改了某个算符的键名，这会导致与添加算符时相同的行为。 删除选择算符的操作会删除从 Deployment 选择算符中删除现有算符。 此操作不需要更改 Pod 模板标签。现有 ReplicaSet 不会被孤立，也不会因此创建新的 ReplicaSet， 但请注意已删除的标签仍然存在于现有的 Pod 和 ReplicaSet 中。 ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:4:2","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"4.3 ReplicaSet 介绍 首先可以看到 nginx 的变化如下图: ReplicaSet 2 扩容到四个节点后: ReplicaSet 4 ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:4:3","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"五、参考 https://kubernetes.io/zh/docs/tasks/run-application/run-stateless-application-deployment/ https://kubernetes.io/zh/docs/concepts/workloads/controllers/deployment/ https://kubernetes.io/zh/docs/tasks/administer-cluster/access-cluster-api/ https://github.com/kubernetes/client-go https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/ ","date":"2020-10-09","objectID":"/2020/10/run-deployment/:5:0","tags":["deployment","pod"],"title":"一拳搞定 Kubernetes | 玩转 deployment","uri":"/2020/10/run-deployment/"},{"categories":["kubernetes"],"content":"本文介绍如何在本机mac运行 Minikube 来安装 kubernetes 集群. ","date":"2020-10-07","objectID":"/2020/10/minikube-install/:0:0","tags":["minikube"],"title":"一拳搞定 Kubernetes | 安装 Minikube","uri":"/2020/10/minikube-install/"},{"categories":["kubernetes"],"content":"一、前言 本篇是整个 kubernetes 的开篇，只有安装好了 kubernetes 集群，我们才能愉快的在容器编排的海洋畅游。 为何选择 minikube？因为它只需要按照官方文档执行简单的命令就可以运行，上手简单，推荐使用。 小贴士 自行备好 docker， linux 等常规知识 ","date":"2020-10-07","objectID":"/2020/10/minikube-install/:1:0","tags":["minikube"],"title":"一拳搞定 Kubernetes | 安装 Minikube","uri":"/2020/10/minikube-install/"},{"categories":["kubernetes"],"content":"二、Minikube 操作 安装参考 ","date":"2020-10-07","objectID":"/2020/10/minikube-install/:2:0","tags":["minikube"],"title":"一拳搞定 Kubernetes | 安装 Minikube","uri":"/2020/10/minikube-install/"},{"categories":["kubernetes"],"content":"2-1. 暂停 Minikube minikube stop 输出: * Stopping \"minikube\" in hyperkit ... * \"minikube\" 已停止 ","date":"2020-10-07","objectID":"/2020/10/minikube-install/:2:1","tags":["minikube"],"title":"一拳搞定 Kubernetes | 安装 Minikube","uri":"/2020/10/minikube-install/"},{"categories":["kubernetes"],"content":"2-2. 启动 minikube start 输出: * Darwin 10.15.6 上的 minikube v1.6.2 - KUBECONFIG=/Users/tc/.kube/config.22 * Selecting 'hyperkit' driver from existing profile (alternates: []) * Tip: Use 'minikube start -p \u003cname\u003e' to create a new cluster, or 'minikube delete' to delete this one. * Starting existing hyperkit VM for \"minikube\" ... * Waiting for the host to be provisioned ... ^[* 正在 Docker '19.03.5' 中准备 Kubernetes v1.17.0… * 正在启动 Kubernetes ... * 完成！kubectl 已经配置至 \"minikube\" ","date":"2020-10-07","objectID":"/2020/10/minikube-install/:2:2","tags":["minikube"],"title":"一拳搞定 Kubernetes | 安装 Minikube","uri":"/2020/10/minikube-install/"},{"categories":["kubernetes"],"content":"2-3. 启动 dashboard minikube dashboard 输出: * Verifying dashboard health ... * Launching proxy ... * Verifying proxy health ... * Opening http://127.0.0.1:58368/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser... 会自动跳转到浏览器： dashboard ","date":"2020-10-07","objectID":"/2020/10/minikube-install/:2:3","tags":["minikube"],"title":"一拳搞定 Kubernetes | 安装 Minikube","uri":"/2020/10/minikube-install/"},{"categories":["kubernetes"],"content":"2-4. 查看 service 的 URL minikube service hello-minikube --url ","date":"2020-10-07","objectID":"/2020/10/minikube-install/:2:4","tags":["minikube"],"title":"一拳搞定 Kubernetes | 安装 Minikube","uri":"/2020/10/minikube-install/"},{"categories":["kubernetes"],"content":"2-5. 删除 Minikube 集群 minikube delete 输出: * Deleting \"minikube\" ... * The \"minikube\" cluster has been deleted. ","date":"2020-10-07","objectID":"/2020/10/minikube-install/:2:5","tags":["minikube"],"title":"一拳搞定 Kubernetes | 安装 Minikube","uri":"/2020/10/minikube-install/"},{"categories":["kubernetes"],"content":"三、kubernetes 操作 创建 deployment kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=NodePort --port=8080 请注意看这个 --type=NodePort，它表示了服务暴露的模式，也就是你要访问到 k8s 里面一个运行程序的途径。 ","date":"2020-10-07","objectID":"/2020/10/minikube-install/:3:0","tags":["minikube"],"title":"一拳搞定 Kubernetes | 安装 Minikube","uri":"/2020/10/minikube-install/"},{"categories":["kubernetes"],"content":"四、补充说明 ","date":"2020-10-07","objectID":"/2020/10/minikube-install/:4:0","tags":["minikube"],"title":"一拳搞定 Kubernetes | 安装 Minikube","uri":"/2020/10/minikube-install/"},{"categories":["kubernetes"],"content":"3-1. 不能暴露 service 当创建的 service 不加 --type=NodePort，在 minikube 获取不到 service url。 minikube service nginx-deployment --namespace=test --url 输出： |-----------|------------------|-------------|--------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|------------------|-------------|--------------| | test | nginx-deployment | | No node port | |-----------|------------------|-------------|--------------| * service test/nginx-deployment has no node port 正常情况输出： http://192.168.64.3:31022 请求这个地址： curl http://192.168.64.3:31022 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e 有输出，表示请求到了 k8s 集群部署的 nginx。 ","date":"2020-10-07","objectID":"/2020/10/minikube-install/:4:1","tags":["minikube"],"title":"一拳搞定 Kubernetes | 安装 Minikube","uri":"/2020/10/minikube-install/"},{"categories":["kubernetes"],"content":"3-2. dashboard 打不开了 在电脑从公司带回家再次来到公司的时候，出现了一些网络问题，比如 dashboard 无法打开，在浏览器回车 dashboard 地址, 类似：http://127.0.0.1:63999/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/#/service?namespace=default 后返回: { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": { }, \"status\": \"Failure\", \"message\": \"no endpoints available for service \\\"http:kubernetes-dashboard:\\\"\", \"reason\": \"ServiceUnavailable\", \"code\": 503 } 这时候建议 stop minikube 后再重新启动. ","date":"2020-10-07","objectID":"/2020/10/minikube-install/:4:2","tags":["minikube"],"title":"一拳搞定 Kubernetes | 安装 Minikube","uri":"/2020/10/minikube-install/"},{"categories":["kubernetes"],"content":"3-3. kubeconfig 小贴士 这份配置在后续 client-go 连接集群的时候也会讲到，是访问集群的身份认证信息配置文件。 如上面启动时候的输出一份配置： - KUBECONFIG=/Users/tc/.kube/config.22 这里 KUBECONFIG 是可以通过环境变量指定的，比如我本地： export KUBECONFIG=/Users/tc/.kube/config.22 里面的内容如下： cat /Users/tc/.kube/config.22 apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURBVENDQWVtZ0F3SUJBZ0lKQU91U0IwYkU3VGIrTUEwR0NTcUdTSWIzRFFFQkN3VUFNQmN4RlRBVEJnTlYKQkFNTURERXdMakUxTWk0eE9ETXVNVEFlRncweE9URXlNekF3TmpVek1qTmFGdzAwTnpBMU1UY3dOalV6TWpOYQpNQmN4RlRBVEJnTlZCQU1NRERFd0xqRTFNaTR4T0RNdU1UQ0NBU0l3RFFZSktvWklodmNOQVFFQkJRQURnZ0VQCkFEQ0NBUW9DZ2dFQkFNazV4T2tXb0RXUzdRdFZhOFFCN21DbFFCeC90N2VHdXpEdENYVXYyWU8wcVBJa3RHRVQKaDZkM2dFQ3kzK0ZVVGhDWTF1U2gzYkhDT2wybXJPWnJmN21DelVPdDJZSWQ3QWFVOEVZQzN6dDl6L2d3NTdBWgpWeHE4L3NYZUNQSlBQZXVpOTR0b252cTBSNXNVMm0veTB3UFJMdTFkS052cUV6K05YNFRPUnpCcVpNeVoycXU0CjNMazJCMkNycXJnOXQ1Wks0cU9wQTMwZUxYNHMwVlZUN042RkdFWlRFMTkzdXNLc2dkV2FGb1pJcnh6LzZDbUUKemlDektMcVZiNzAxdWJVZXlCTEU0OER4cVVyZTdtYkoxSWI0VUU5NjE1cDhuSml2UlRJdXZ5aUlxazFieEQ5RwpkaVNEMTdUV1NpTnUwelUyQkJWaEV4OTRYbDcwcENucnhxRUNBd0VBQWFOUU1FNHdIUVlEVlIwT0JCWUVGS29jClFUVE1RYTF1TzZMVk1nVEd0WndsMFNxek1COEdBMVVkSXdRWU1CYUFGS29jUVRUTVFhMXVPNkxWTWdUR3Rad2wKMFNxek1Bd0dBMVVkRXdRRk1BTUJBZjh3RFFZSktvWklodmNOQVFFTEJRQURnZ0VCQUZodlYwVkxGMlRYU0hTLwpGeXBQNG1jMy8vWG9RdWRrUUdNSTJQWDNmNk8xWlBteFVrOGpiMnhiUmJ0aGhTcU5BU0xPSGdSQnRqYjF1UnVaCi96NVliRnhpTEFXdWYyd3FoSzJtRkl4WHg2enF1bExremszRFMxRjZnVWxhYS9hRXoxWW5XdWhUdzArenRtYkkKQkdENnRaTUM3ZDA1YlhseTREQkNtZklPclNNUW14eURvN0IxV1RHWklpd2NwSzN5clJsUEFCc2w5dnJUcit0RQoydUwxaytoYnB3ZjRreVZrNzNRZWdWTGZaK3Q3ZUZRVUpCMDR2V1RSdmtQcVdoTWhqQXRTR1FObG80TVJ3eklWCit2TDUyVGcxZjNXb09LRldyQkQzM1MzOGhLWlpJbzVKUDlPMmxDUU9VRXhRVzZEcmN0UEp0bmhRbHdEUjQ0a04KL1FxNmFNZz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= server: https://30.11.44.31:16443 name: microk8s-cluster - cluster: certificate-authority: /Users/tc/.minikube/ca.crt server: https://192.168.64.3:8443 name: minikube contexts: - context: cluster: microk8s-cluster user: admin name: microk8s - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: admin user: password: d0F2R1hDc2RYUEgxWFA4RGtpWVdFQldHM2xrU0Q2M3czZ3ovK3YzNXJxaz0K username: admin - name: minikube user: client-certificate: /Users/tc/.minikube/client.crt client-key: /Users/tc/.minikube/client.key 也可以执行命令 kubectl config view 是一样的效果。 如果要访问多个 k8s 集群，可以通过在 kubectl 命令加上 --kubeconfig=[位置] 来实现。 ","date":"2020-10-07","objectID":"/2020/10/minikube-install/:4:3","tags":["minikube"],"title":"一拳搞定 Kubernetes | 安装 Minikube","uri":"/2020/10/minikube-install/"},{"categories":["kubernetes"],"content":"五、参考 https://kubernetes.io/zh/docs/setup/learning-environment/minikube/ https://minikube.sigs.k8s.io/docs/start/ https://kubernetes.io/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/ ","date":"2020-10-07","objectID":"/2020/10/minikube-install/:5:0","tags":["minikube"],"title":"一拳搞定 Kubernetes | 安装 Minikube","uri":"/2020/10/minikube-install/"},{"categories":["算法"],"content":"本文介绍 TalkGo 算法之美第一期题解 ","date":"2020-09-18","objectID":"/2020/09/talkgo-algorithms-01/:0:0","tags":["算法","TalkGo"],"title":"TalkGo 算法之美第一期","uri":"/2020/09/talkgo-algorithms-01/"},{"categories":["算法"],"content":"一、前言 talkgo地址 题目一 输入一个递增排序的数组和一个数字 S，在数组中查找两个数，使得他们的和正好是 S，如果有多对数字的和等于 S，输出两个数的乘积最小的。 LeetCode 题目二 一个有序数组，从随即一位截断，把前段放在后边，如 4 5 6 7 1 2 3 求中位数 ","date":"2020-09-18","objectID":"/2020/09/talkgo-algorithms-01/:1:0","tags":["算法","TalkGo"],"title":"TalkGo 算法之美第一期","uri":"/2020/09/talkgo-algorithms-01/"},{"categories":["算法"],"content":"二、解析 ","date":"2020-09-18","objectID":"/2020/09/talkgo-algorithms-01/:2:0","tags":["算法","TalkGo"],"title":"TalkGo 算法之美第一期","uri":"/2020/09/talkgo-algorithms-01/"},{"categories":["算法"],"content":"2.1 题目一 思考： 因为递增，负数和正数的情况一致，一组数据最小的值越小则乘积也越小 如果出现重复数字，也和正常情况一样的处理 2.1.1 处理方式 暴力法 func TwoSum2For(sum int, array []int) []int { b, arr := checkArray(array) if !b { return arr } for i := 0; i \u003c len(array); i++ { for j := len(array) - 1; j \u003e i; j-- { if sum == array[i]+array[j] { return []int{array[i], array[j]} } } } return []int{} } 直接两层 for 循环，如何匹配到数组两个位置的值等于传入的和，则直接返回。此方法简单易懂，可读性高。 时间复杂度 O(n^2) 空间复杂度 O(1) 缓存法 // TwoSumMap map 缓存法 func TwoSumMap(sum int, array []int) []int { b, arr := checkArray(array) if !b { return arr } // 通过 map 缓存数据 cache := make(map[int]int, len(array)) for i := range array { cache[array[i]] = i } // 返回值在数组中的位置 for i := range array { // 通过减法找到另一个值，直接去 map 查找 if _, ok := cache[sum-array[i]]; ok { return []int{array[i], sum - array[i]} } } return []int{} } 借助 map 把数组的值进行了缓存，用到了加法变减法的思路根据 hash 算法可以直接找到对应的另一个值是否存在。这个写法很直观，比第一种暴力法更加简单，建议平日小数量的时候直接用就行。 时间复杂度 O(n) 空间复杂度 O(2n) 双指针法 func TwoSum2Point(sum int, array []int) []int { b, arr := checkArray(array) if !b { return arr } left, right := 0, len(array)-1 for left \u003c right { s := array[left] + array[right] if sum == s { return []int{array[left], array[right]} } else if sum \u003e s { left++ } else { right-- } } return []int{} } 双指针法是字符查找的时候经常使用的方法，在数据量大的场景下会比较有优势。它的写法代码看着比较高级。 ","date":"2020-09-18","objectID":"/2020/09/talkgo-algorithms-01/:2:1","tags":["算法","TalkGo"],"title":"TalkGo 算法之美第一期","uri":"/2020/09/talkgo-algorithms-01/"},{"categories":["算法"],"content":"2.2 题目二 思考： 先要回想下什么是中位数，奇偶的处理方式是不一样的 找到截断的位置，计算中位数涉及到的数字的 index 偶数中位数的一个例子： 偶数 中位数在线学习 代码： // MiddleNum 算中位数 func MiddleNum(nums []int) (float32, error) { l := len(nums) if l \u003c= 0 { return 0, illegalArray } if l == 1 { return float32(nums[0]), nil } else if l == 2 { return float32((nums[0] + nums[1]) / 2), nil } p := Point(nums) if isEven(l) { target1 := l / 2 target2 := l/2 + 1 return (float32(nums[Position(l, p, target1)]) + float32(nums[Position(l, p, target2)])) / 2, nil } else { return float32(nums[Position(l, p, (l+1)/2)]), nil } } // Point 得到拐点 func Point(nums []int) int { if len(nums) \u003c 3 { return 0 } n, flag := nums[0], -1 for i := 1; i \u003c len(nums); i++ { cf := -1 c := nums[i] if c \u003c n { cf = 1 // 降 } else if c \u003e n { cf = 0 // 升 } if flag \u003e= 0 \u0026\u0026 cf \u003e= 0 \u0026\u0026 cf != flag { return i } n = c if cf \u003e= 0 { flag = cf } } return 0 } // Position 获取位置 func Position(l, p, t int) int { if l-p \u003e t { return p + t - 1 } else { return t + p - l - 1 } } ","date":"2020-09-18","objectID":"/2020/09/talkgo-algorithms-01/:2:2","tags":["算法","TalkGo"],"title":"TalkGo 算法之美第一期","uri":"/2020/09/talkgo-algorithms-01/"},{"categories":["算法"],"content":"代码 题1 题2 ","date":"2020-09-18","objectID":"/2020/09/talkgo-algorithms-01/:3:0","tags":["算法","TalkGo"],"title":"TalkGo 算法之美第一期","uri":"/2020/09/talkgo-algorithms-01/"},{"categories":["领域驱动"],"content":"本文介绍一个实际项目中可以使用的概念: Domain Primitive. ","date":"2020-05-28","objectID":"/2020/05/domain-primitive/:0:0","tags":["DDD","值对象"],"title":"领域驱动实战 | Domain Primitive 的简单使用","uri":"/2020/05/domain-primitive/"},{"categories":["领域驱动"],"content":"一、前言 领域驱动这个概念也听到很多次了,书上看过的已经忘记,道听途说的不敢苟同（自己也分不清真假,听闻也没一个真正”成功“的案例）,这里我就说一句我的理解:深入挖掘业务内核,找到问题的根本,把复杂问题分为多个领域（领域内聚）去解决业务实际的问题,而不是通过技术角度随着岁月留下一堆跳跃的代码. ","date":"2020-05-28","objectID":"/2020/05/domain-primitive/:1:0","tags":["DDD","值对象"],"title":"领域驱动实战 | Domain Primitive 的简单使用","uri":"/2020/05/domain-primitive/"},{"categories":["领域驱动"],"content":"二、本文主角 Domain Primitive ","date":"2020-05-28","objectID":"/2020/05/domain-primitive/:2:0","tags":["DDD","值对象"],"title":"领域驱动实战 | Domain Primitive 的简单使用","uri":"/2020/05/domain-primitive/"},{"categories":["领域驱动"],"content":"2.1 Domain Primitive 的定义 概念 不从任何其他事物发展而来 初级的形成或生长的早期阶段 让我们重新来定义一下 Domain Primitive :Domain Primitive 是一个在特定领域里,拥有精准定义的、可自我验证的、拥有行为的 Value Object . • DP是一个传统意义上的Value Object,拥有Immutable的特性 • DP是一个完整的概念整体,拥有精准定义 • DP使用业务域中的原生语言 • DP可以是业务域的最小组成部分、也可以构建复杂组合 注意:Domain Primitive的概念和命名来自于Dan Bergh Johnsson \u0026 Daniel Deogun的书 Secure by Design. ","date":"2020-05-28","objectID":"/2020/05/domain-primitive/:2:1","tags":["DDD","值对象"],"title":"领域驱动实战 | Domain Primitive 的简单使用","uri":"/2020/05/domain-primitive/"},{"categories":["领域驱动"],"content":"2.2 使用 Domain Primitive 的三原则 • 让隐性的概念显性化 • 让隐性的上下文显性化 • 封装多对象行为 ","date":"2020-05-28","objectID":"/2020/05/domain-primitive/:2:2","tags":["DDD","值对象"],"title":"领域驱动实战 | Domain Primitive 的简单使用","uri":"/2020/05/domain-primitive/"},{"categories":["领域驱动"],"content":"2.3 Domain Primitive 和 DDD 里 Value Object 的区别 在 DDD 中, Value Object 这个概念其实已经存在: • 在 Evans 的 DDD 蓝皮书中,Value Object 更多的是一个非 Entity 的值对象 • 在Vernon的IDDD红皮书中,作者更多的关注了Value Object的Immutability、Equals方法、Factory方法等 Domain Primitive 是 Value Object 的进阶版,在原始 VO 的基础上要求每个 DP 拥有概念的整体,而不仅仅是值对象.在 VO 的 Immutable 基础上增加了 Validity 和行为.当然同样的要求无副作用（side-effect free）. ▍Domain Primitive 和 Data Transfer Object (DTO) 的区别 在日常开发中经常会碰到的另一个数据结构是 DTO ,比如方法的入参和出参.DP 和 DTO 的区别如下: DTO DP 功能 数据传输属于技术细节 业务领域中的概念 数据的关联 只是一堆数据放在一起不一定有关联度 数据之间的高相关新 行为 无行为 丰富的行为和业务逻辑 ","date":"2020-05-28","objectID":"/2020/05/domain-primitive/:2:3","tags":["DDD","值对象"],"title":"领域驱动实战 | Domain Primitive 的简单使用","uri":"/2020/05/domain-primitive/"},{"categories":["领域驱动"],"content":"2.4 什么情况下应该用 Domain Primitive 常见的 DP 的使用场景包括: • 有格式限制的 String:比如Name,PhoneNumber,OrderNumber,ZipCode,Address等 • 有限制的Integer:比如OrderId（\u003e0）,Percentage（0-100%）,Quantity（\u003e=0）等 • 可枚举的 int :比如 Status（一般不用Enum因为反序列化问题） • Double 或 BigDecimal:一般用到的 Double 或 BigDecimal 都是有业务含义的,比如 Temperature、Money、Amount、ExchangeRate、Rating 等 • 复杂的数据结构:比如 Map\u003e 等,尽量能把 Map 的所有操作包装掉,仅暴露必要行为 ","date":"2020-05-28","objectID":"/2020/05/domain-primitive/:2:4","tags":["DDD","值对象"],"title":"领域驱动实战 | Domain Primitive 的简单使用","uri":"/2020/05/domain-primitive/"},{"categories":["领域驱动"],"content":"三、项目的的尝试 举例子: DP对象 @Getter @JSONType(deserializer = NameDeserializer.class, serializer = NameSerializer.class) public class Name implements Serializable { private static final long serialVersionUID = 7482387369308807214L; private final String name; public Name(final String name) { if (StringUtils.isBlank(name)) { throw new ValidationException(\"名称不能为空!!!\"); } if (!isValid(name)) { throw new ValidationException(\"仅支持1-64位大小写字母,数字,中划线和下划线组成,必须字母开头!!!\"); } this.name = name; } private static boolean isValid(String code) { String pattern = \"^[a-zA-Z][a-zA-Z0-9_-]{1,64}$\"; return code.matches(pattern); } } 领域方法 // 根据应用和Git信息查询流水线发布记录 public PipelineDO queryByAppAndGit(final Name appName, final Git git) { // 进行逻辑处理 } 说明 final 修饰字段,不可变性的特点 构造方法就直接校验合法性,不需要factory和validutil,更内聚自己的功能 因为存在JSON序列化和反序列的情况,这里需要自定义序列化和反序列的方法 效果 前端联调企图传随便的内容进来,直接通不过参数校验,就进不到核心方法了 参数相对更能直观其意,并且进来的参数值取出来肯定是合法的 代码可以写的更少 容易出错的地方 如果用Map做一个缓存,Key放String,而实际对应的是Code对象,那么查询的时候要用code.genCodeString(),一开始容易写code忘记转换,导致查询不到结果 JSON转换在上面已经提到了需要自己定义扩展,会增加一定的重复代码量（我们暂时用的是每个DP类型两个转换类,可以合并到一个方法,根据类型判断,但是语义不够清晰 还没做好的地方 如果通过jar包提供给第三方接口调用也使用这个DP类,那么接口调用方传参不合法的时候就会报错 ","date":"2020-05-28","objectID":"/2020/05/domain-primitive/:3:0","tags":["DDD","值对象"],"title":"领域驱动实战 | Domain Primitive 的简单使用","uri":"/2020/05/domain-primitive/"},{"categories":["领域驱动"],"content":"四、参考 阿里云领域驱动DP文章 ","date":"2020-05-28","objectID":"/2020/05/domain-primitive/:4:0","tags":["DDD","值对象"],"title":"领域驱动实战 | Domain Primitive 的简单使用","uri":"/2020/05/domain-primitive/"},{"categories":["设计模式"],"content":"本文介绍如何在 go 语言中使用模板模式. ","date":"2019-11-09","objectID":"/2019/11/pattern-template/:0:0","tags":["golang","模板模式"],"title":"模板模式","uri":"/2019/11/pattern-template/"},{"categories":["设计模式"],"content":"一、前言 Template Pattern（模板模式） 白话文: 定一个“抽象类”,定义一个方法A,定义需要子类实现的方法,所有子类对象在执行A的时候,会调用各自实现的方法. 在golang中,由于不存在抽象类和真正的继承,所以只能通过一个基础类来充当抽象类,子类通过组合基础类来实现通用方法的继承. 故事: 阳光明媚的一天,我家来了位香港的朋友（毕竟那边太乱）,我们决定一起做一桌菜,于是他做香港菜,我做杭州菜,比拼就这么开始了. ","date":"2019-11-09","objectID":"/2019/11/pattern-template/:1:0","tags":["golang","模板模式"],"title":"模板模式","uri":"/2019/11/pattern-template/"},{"categories":["设计模式"],"content":"二、实例 ","date":"2019-11-09","objectID":"/2019/11/pattern-template/:2:0","tags":["golang","模板模式"],"title":"模板模式","uri":"/2019/11/pattern-template/"},{"categories":["设计模式"],"content":"2.1 普通例子 代码 package main import ( \"fmt\" \"testing\" ) type Cooking interface { DoOperate() } type AbstractCooking struct { Cooking Prepare func() GetContent func() string } func (d AbstractCooking) DoOperate() { d.Prepare() fmt.Println(\"烹饪内容:\", d.GetContent()) fmt.Println(\"烹饪完成\") } type HZCooking struct { AbstractCooking } func NewHZCooking() *HZCooking { c := new(HZCooking) c.AbstractCooking.GetContent = c.GetContent c.AbstractCooking.Prepare = c.Prepare return c } func (c *HZCooking) GetContent() string { return \"杭州菜.\" } func (c *HZCooking) Prepare() { fmt.Println(\" -- 准备杭州菜 -- \") } type HkCooking struct { HZCooking } func NewHKCooking() *HkCooking { c := new(HkCooking) c.AbstractCooking.GetContent = c.GetContent c.AbstractCooking.Prepare = c.Prepare return c } func (c *HkCooking) GetContent() string { return \"香港菜.\" } func (c *HkCooking) Prepare() { fmt.Println(\" -- 准备香港菜 -- \") } func TestCooking(t *testing.T) { chinaCooking := NewHZCooking() chinaCooking.DoOperate() hkCooking := NewHKCooking() hkCooking.DoOperate() } 输出结果: -- 准备杭州菜 -- 烹饪内容: 杭州菜. 烹饪完成 -- 准备香港菜 -- 烹饪内容: 香港菜. 烹饪完成 ","date":"2019-11-09","objectID":"/2019/11/pattern-template/:2:1","tags":["golang","模板模式"],"title":"模板模式","uri":"/2019/11/pattern-template/"},{"categories":["设计模式"],"content":"三、参考 https://www.tutorialspoint.com/design_pattern/template_pattern.htm ","date":"2019-11-09","objectID":"/2019/11/pattern-template/:3:0","tags":["golang","模板模式"],"title":"模板模式","uri":"/2019/11/pattern-template/"},{"categories":["Service Mesh"],"content":"本文介绍 敖小剑大佬在 QCon 上关于 Service Mesh 的分享。 2019 年，蚂蚁金服在 Service Mesh 领域继续高歌猛进，进入大规模落地的深水区。本文整理自蚂蚁金服高级技术专家敖小剑在 QCon 全球软件开发大会（上海站）2019 上的演讲，他介绍了 Service Mesh 在蚂蚁金服的落地情况和即将来临的双十一大考，以及大规模落地时遇到的困难和解决方案，助你了解 Service Mesh 的未来发展方向和前景。 ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:0:0","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"前 言 大家好，我是敖小剑，来自蚂蚁金服中间件团队，今天带来的主题是“诗和远方：蚂蚁金服 Service Mesh 深度实践”。 在过去两年，我先后在 QCon 做过两次 Service Mesh 的演讲： 2017 年，当时 Service Mesh 在国内还属于蛮荒时代，我当时做了一个名为“Service Mesh: 下一代微服务”的演讲，开始在国内布道 Service Mesh 技术； 2018 年，做了名为“长路漫漫踏歌而行：蚂蚁金服 Service Mesh 实践探索”的演讲，介绍蚂蚁金服在 Service Mesh 领域的探索性的实践，当时蚂蚁金服刚开始在 Service Mesh 探索。 今天，有幸第三次来到 QCon，给大家带来的依然是蚂蚁金服在 Service Mesh 领域的实践分享。和去年不同的是，今年蚂蚁金服进入了 Service Mesh 落地的深水区，规模巨大，而且即将迎来双十一大促考验。 备注：现场做了一个调研，了解听众对 Servicve Mesh 的了解程度，结果不太理想：在此之前对 Service Mesh 有了解的同学目测只有 10% 多点（肯定不到 20%）。Service Mesh 的技术布道，依然任重道远。 今天给大家带来的内容主要有三块： 蚂蚁金服落地情况介绍：包括大家最关心的双十一落地情况； 大规模落地的困难和挑战：分享一下我们过去一年中在大规模落地上遇到的问题； 是否采用 Service Mesh 的建议：这个问题经常被人问起，所以借这个机会给出一些中肯的建议供大家参考； ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:1:0","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"蚂蚁金服落地情况介绍 发展历程和落地规模 Service Mesh 技术在蚂蚁金服的落地，先后经历过如下几个阶段： 技术预研 阶段：2017 年底开始调研并探索 Service Mesh 技术，并确定为未来发展方向； 技术探索 阶段：2018 年初开始用 Golang 开发 Sidecar SOFAMosn，年中开源基于 Istio 的 SOFAMesh； 小规模落地 阶段：2018 年开始内部落地，第一批场景是替代 Java 语言之外的其他语言的客户端 SDK，之后开始内部小范围试点； 规模落地 阶段：2019 年上半年，作为蚂蚁金融级云原生架构升级的主要内容之一，逐渐铺开到蚂蚁金服内部的业务应用，并平稳支撑了 618 大促； 全面大规模落地 阶段：2019 年下半年，在蚂蚁金服内部的业务中全面铺开，落地规模非常庞大，而且准备迎接双十一大促； 目前 ServiceMesh 正在蚂蚁金服内部大面积铺开，我这里给出的数据是前段时间（大概 9 月中）在云栖大会上公布的数据：应用数百个，容器数量（pod 数）超过 10 万。当然目前落地的 pod 数量已经远超过 10 万，这已经是目前全球最大的 Service Mesh 集群，但这仅仅是一个开始，这个集群的规模后续会继续扩大，明年蚂蚁金服会有更多的应用迁移到 Service Mesh。 ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:1:1","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"主要落地场景 目前 Service Mesh 在蚂蚁金服内部大量落地，包括支付宝的部分核心链路，落地的主要场景有： 多语言支持：目前除了支持 Java 之外，还支持 Golang，Python，C++，NodeJS 等语言的相互通信和服务治理； 应用无感知的升级：关于这一点我们后面会有特别的说明； 流量控制：经典的 Istio 精准细粒度流量控制； RPC 协议支持：和 Istio 不同，我们内部使用的主要是 RPC 协议； 可观测性； ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:1:2","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"Service Mesh 的实际性能数据 之前和一些朋友、客户交流过，目前在 Service Mesh 方面大家最关心的是 Service Mesh 的性能表现，包括对于这次蚂蚁金服 Service Mesh 上双十一，大家最想看到的也是性能指标。 为什么大家对性能这么关注？ 因为在 Service Mesh 工作原理的各种介绍中，都会提到 Service Mesh 是将原来的一次远程调用，改为走 Sidecar（而且像 Istio 是客户端和服务器端两次 Sidecar，如上图所示），这样一次远程调用就会变成三次远程调用，对性能的担忧也就自然而然的产生了：一次远程调用变三次远程调用，性能会下降多少？延迟会增加多少？ 下图是我们内部的大促压测数据，对比带 SOFAMosn 和不带 SOFAMosn 的情况（实现相同的功能）。其中 SOFAMosn 是我们蚂蚁金服自行开发的基于 Golang 的 Sidecar/ 数据平面，我们用它替代了 Envoy，在去年的演讲中我有做过详细的介绍。 SOFAMosn： https://github.com/sofastack/sofa-mosn CPU：CPU 使用在峰值情况下增加 8%，均值约增加 2%。在最新的一次压测中，CPU 已经优化到基本持平（低于 1%）； 内存：带 SOFAMosn 的节点比不带 SOFAMosn 的节点内存占用平均多 15M； 延迟：延迟增加平均约 0.2ms。部分场景带 SOFAMosn 比不带 SOFAMosn RT 增加约 5%，但是有部分特殊场景带 SOFAMosn 比不带 SOFAMosn RT 反而降低 7.5%； 这个性能表现，和前面\"一次远程调用变三次远程调用\"的背景和担忧相比有很大的反差。尤其是上面延迟的这个特殊场景，居然出现带 SOFAMosn（三次远程调用）比不带 SOFAMosn（一次远程调用） 延迟反而降低的情况。 是不是感觉不科学？ ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:2:0","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"Service Mesh 的基本思路 我们来快速回顾一下 Service Mesh 实现的基本思路： 在基于 SDK 的方案中，应用既有业务逻辑，也有各种非业务功能。虽然通过 SDK 实现了代码重用，但是在部署时，这些功能还是混合在一个进程内的。 在 Service Mesh 中，我们将 SDK 客户端的功能从应用中剥离出来，拆解为独立进程，以 Sidecar 的模式部署，让业务进程专注于业务逻辑： 业务进程：专注业务实现，无需感知 Mesh； Sidecar 进程：专注服务间通讯和相关能力，与业务逻辑无关； 我们称之为\"关注点分离\"：业务开发团队可以专注于业务逻辑，而底层的中间件团队（或者基础设施团队）可以专注于业务逻辑之外的各种通用功能。 通过 Sidecar 拆分为两个独立进程之后，业务应用和 Sidecar 就可以实现“独立维护”：我们可以单独更新 / 升级业务应用或者 Sidecar。 ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:2:1","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"性能数据背后的情景分析 我们回到前面的蚂蚁金服 Service Mesh 落地后的性能对比数据：从原理上说，Sidecar 拆分之后，原来 SDK 中的各种功能只是拆分到 Sidecar 中。整体上并没有增减，因此理论上说 SDK 和 Sidecar 性能表现是一致的。由于增加了应用和 Sidecar 之间的远程调用，性能不可避免的肯定要受到影响。 首先我们来解释第一个问题：为什么性能损失那么小，和\"一次远程调用变三次远程调用\"的直觉不符？ 所谓的“直觉”，是将关注点都集中到了远程调用开销上，下意识的忽略了其他开销，比如 SDK 的开销、业务逻辑处理的开销，因此： 推导出来的结果就是有 3 倍的开销，性能自然会有非常大的影响。 但是，真实世界中的应用不是这样： 业务逻辑的占比很高：Sidecar 转发的资源消耗相比之下要低很多，通常是十倍百倍甚至千倍的差异； SDK 也是有消耗的：即使不考虑各种复杂的功能特性，仅仅就报文（尤其是 Body）序列化的编解码开销也是不低的。而且，客户端和服务器端原有的编解码过程是需要处理 Body 的，而在 Sidecar 中，通常都只是读取 Header 而透传 Body，因此在编解码上要快很多。另外应用和 Sidecar 的两次远程通讯，都是走的 Localhost 而不是真实的网络，速度也要快非常多； 因此，在真实世界中，我们假定业务逻辑百倍于 Sidecar 的开销，而 SDK 十倍于 Sidecar 的开销，则： 推导出来的结果，性能开销从 111 增加到 113，大约增加 2%。这也就解释了为什么我们实际给出的 Service Mesh 的 CPU 和延迟的性能损失都不大的原因。当然，这里我是刻意选择了 100 和 10 这两个系数来拼凑出 2% 这个估算结果，以迎合我们前面给出“均值约增加 2%”的数据。这不是准确数值，只是用来模拟。 ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:2:2","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"情理当中的意外惊喜 前面的分析可以解释性能开销增加不多的情景，但是，还记得我们的数据中有一个不科学的地方吗：“部分特殊场景带 SOFAMosn 比不带 SOFAMosn RT 反而降低 7.5%”。 理论上，无论业务逻辑和 SDK 的开销比 Sidecar 的开销大多少，也就是不管我们怎么优化 Sidecar 的性能，其结果也只能接近零。无论如何不可能出现多两个 Sidecar，CPU 消耗和延迟反而降低的情况。 这个“不科学”是怎么出现的？ 我们继续来回顾这个 Service Mesh 的实现原理图： 出现性能大幅提升的主要的原因，是我们在 SOFAMosn 上做了大量的优化，特别是路由的缓存。在蚂蚁金服内部，服务路由的计算和处理是一个异常复杂的逻辑，非常耗资源。而在最近的优化中，我们为服务路由增加了缓存，从而使得服务路由的性能得到了大幅提升。因此： 备注：这里我依然是刻意拼凑出 -7% 这个估算结果，请注意这不是准确数值，只是用来模拟示意。 也许有同学会说，这个结果不“公平”：这是优化了的服务路由实现在 PK 没有优化的服务路由实现。的确，理论上说，在 Sidecar 中做的任何性能优化，在 SDK 里面同样可以实现。但是，在 SDK 上做的优化需要等整个调用链路上的应用全部升级到优化后的 SDK 之后才能完全显现。而在传统 SDK 方案中，SDK 的升级是需要应用配合，这通常是一个漫长的等待过程。很可能代码优化和发版一周搞定，但是让全站所有应用都升级到新版本的 SDK 要花费数月甚至一年。 此时 Service Mesh 的优点就凸显出来了：Service Mesh 下，业务应用和 Sidecar 可以“独立维护” ，我们可以很方便的在业务应用无感知的情况下升级 Sidecar。因此，任何 Sidecar 的优化结果，都可以非常快速的获取收益，从而推动我们对 Sidecar 进行持续不断的升级。 前面这个延迟降低 7% 的例子，就是一个非常经典的故事：在中秋节前后，我们开发团队的同学，不辞辛苦加班加点的进行压测和性能调优，在一周之内连续做了多次性能优化，连发了多个性能优化的小版本，以“小步快跑”的方式，最后拿到了这个令大家都非常开心的结果。 总结：持续不断的优化 + 无感知升级 = 快速获得收益 这是一个意外惊喜，但又在情理之中：这是 SDK 下沉到基础设施并具备独立升级能力后带来的红利。 也希望这个例子，能够让大家更深刻的理解 Service Mesh 的基本原理和优势。 ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:2:3","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"大规模落地的困难和挑战 当 Service Mesh 遇到蚂蚁金服的规模，困难和挑战也随之而来：当规模达到一定程度时，很多原本很小的问题都会急剧放大。后面我将在性能、容量、稳定性、可维护性和应用迁移几个方面给大家介绍我们遇到的挑战和实践。 ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:3:0","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"数据平面的优化 在数据平面上，蚂蚁金服采用了自行研发的基于 Golang 的方案：SOFAMosn。关于为什么选择全新开发 SOFAMosn，而不是直接使用 Envoy 的原因，在去年 QCon 的演讲中我有过详细的介绍，有兴趣可以了解。 前面我们给出的性能数据，实际上主要是数据平面的性能，也就是作为 Sidecar 部署的 SOFAMosn 的性能表现。从数据上看 SOFAMosn 目前的性能表现还是很不错的，这背后是我们在 SOFAMosn 上做了非常多的性能优化。 CPU 优化：在 SOFAMosn 中我们进行了 Golang 的 writev 优化，将多个包拼装一次写以降低 syscall 调用。测试中发现，Golang 1.9 的时候 writev 有内存泄露的 bug。当时 debug 的过程非常的辛苦…… 详情见我们当时给 Golang 提交的 PR： https://github.com/golang/go/pull/32138； 内存优化：在内存复用，我们发现报文直接解析会产生大量临时对象。SOFAMosn 通过直接复用报文字节的方式，将必要的信息直接通过 unsafe.Pointer 指向报文的指定位置来避免临时对象的产生； 延迟优化：前面我们谈到 Sidecar 是通过只解析 Header 而透传 Body 来保证性能的。针对这一点，我们进行了协议升级，以便快速读取 Header。比如我们使用的 TR 协议请求头和 Body 均为 hessian 序列化，性能损耗较大。而 Bolt 协议中 Header 是一个扁平化 map，解析性能损耗小。因此我们升级应用改走 Bolt 协议来提升 Sidecar 转发的性能。这是一个典型的针对 Sidecar 特性而做的优化； 此外还有前面特意重点介绍的路由缓存优化（也就是那个不科学的延迟降低 7% 的场景）。由于蚂蚁金服内部路由的复杂性（一笔请求经常需要走多种路由策略最终确定路由结果目标），通过对相同条件的路由结果做秒级缓存，我们成功将某核心链路的全链路 RT 降低 7%。 这里我简单给出了上述几个典型案例，双十一之后会有更多更详细的 SOFAMosn 资料分享出来，有兴趣的同学可以多关注。 在双十一过后，我们也将加大 SOFAMosn 在开源上的投入，将 SOFAMosn 做更好地模块化地抽象，并且将双十一中经过考验的各种优化放进去，预计在 2020 年的 1 月底可以发布第一个优化后的版本。 ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:3:1","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"Mixer 的性能优化 vMixer 的性能优化是个老生常谈的话题，基本上只要谈及 Istio 的性能，都避无可避。 Mixer 的性能问题，一直都是 Istio 中最被人诟病的地方。 尤其在 Istio 1.1/1.2 版本之后，引入 Out-Of-Process Adapter 之后，更是雪上加霜。 原来 Sidecar 和 Mixer 之间的远程调用已经严重影响性能，在引入 Out-Of-Process Adapter 之后又在 Traffic 流程中引入了新的远程调用，性能更加不可接受。 从落地的角度看，Mixer V1 糟糕至极的性能，已经是“生命无法承受之重”。对于一般规模的生产级落地而言，Mixer 性能已经是难于接受，更不要提大规模落地…… Mixer V2 方案则给了社区希望：将 Mixer 合并进 Sidecar，引入 web assembly 进行 Adapter 扩展，这是我们期待的 Mixer 落地的正确姿势，是 Mixer 的未来，是 Mixer 的\"诗和远方\"。然而社区望穿秋水，但 Mixer V2 迟迟未能启动，长期处于 In Review 状态，远水解不了近渴。 因此在 Mixer 落地上，我们只能接受妥协方案，所谓\"眼前的苟且\"：一方面我们弃用 Mixer v1，改为在 SOFAMosn 中直接实现功能；另一方面我们并没有实现 Mixer V2 的规划。实际的落地方式是：我们只在 SOFAMosn 中提供最基本的策略检查功能如限流，鉴权等，另外可观测性相关的各种能力也都是从 SOFAMosn 直接输出。 ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:3:2","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"Pilot 的性能优化 在 Istio 中，Pilot 是一个被 Mixer 掩盖的重灾区：长期以来大家的性能关注点都在 Mixer，表现糟糕而且问题明显的 Mixer 一直在吸引火力。但是当选择放弃 Mixer（典型如官方在 Istio 新版本中提供的关闭 Mixer 的配置开关）之后，Pilot 的性能问题也就很快浮出水面。 这里简单展示一下我们在 Pilot 上做的部分性能优化： 序列化优化：我们全面使用 types.Any 类型，弃用 types.Struct 类型，序列化性能提升 70 倍，整体性能提升 4 倍。Istio 最新的版本中也已经将默认模式修改为 types.Any 类型。我们还进行了 CR(CustomResource) 的序列化缓存，将序列化时机从 Get/List 操作提前至事件触发时，并缓存结果。大幅降低序列化频率，压测场景下整体性能提升 3 倍，GC 频率大幅下降； 预计算优化：支持 Sidecar CRD 维度的 CDS /LDS/RDS 预计算，大幅降低重复计算，压测场景下整体性能提升 6 倍；支持 Gateway 维度的 CDS / LDS / RDS 预计算；计算变更事件的影响范围，支持局部推送，减少多余的计算和对无关 Sidecar 的打扰； 推送优化：支持运行时动态降级，支持熔断阈值调整，限流阈值调整，静默周期调整，日志级别调整；实现增量 ADS 接口，在配置相关处理上，Sidecar cpu 减少 90%，Pilot cpu 减少 42%； 这里简单解释一下，Pilot 在推送数据给 Sidecar 时，代码实现上的有些简单：Sidecar 连接上 Pilot 时；Pilot 就给 Sidecar 下发 xDS 数据。假定某个服务有 100 个实例，则这 100 个实例的 Sidecar 连接到 Pilot 时，每次都会进行一次下发数据的计算，然后进行序列化，再下发给连上来的 Sidecar。下一个 Sidecar 连接上来时，重复这些计算和序列化工作，而不管下发的数据是否完全相同，我们称之为“千人千面”。 而实际中，同一个服务往往有多个实例，Pilot 下发给这些实例的 Sidecar 的数据往往是相同的。因此我们做了优化，提前做预计算和序列化并缓存结果，以便后续重复的实例可以直接从缓存中取。因此，“千人千面”就可以优化为“千人百面”或者“千人十面”，从而大幅提高性能。 另外，对于整个 Service Mesh 体系，Pilot 至关重要。因此 Pilot 本身也应该进行保护，也需要诸如熔断 / 限流等特性。 ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:3:3","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"Service Mesh 的运维 在 Service Mesh 的运维上，我们继续坚持“线上变更三板斧”原则。这里的变更，包括发布新版本，也包括修改配置，尤其特指修改 Istio 的 CRD。 线上变更“三板斧”指的是： 可灰度：任何变更，都必须是可以灰度的，即控制变更的生效范围。先做小范围内变更，验证通过之后才扩大范围； 可监控：在灰度过程中，必须能做到可监控，能了解到变更之后对系统的应用。如果没有可监控，则可灰度也就没有意义了； 可回滚：当通过监控发现变更后会引发问题时，还需要有方法可以回滚； 我们在这里额外引入了一个名为“ScopeConfig”的配置变更生效范围的控制能力，即配置变更的灰度。什么是配置变更的灰度呢？ Istio 的官方实现，默认修改配置（Istio API 对应的各种 CRD）时新修改的配置会直接全量推动到所有生效的 Sidecar，即配置变更本身无法灰度。注意这里和平时说的灰度不同，比如最常见的场景，服务 A 调用服务 B，并假定服务 A 有 100 个实例，而服务 B 有 10 个 v1 版本的服务实例正在进行。此时需要更新服务 B 到新的 v2 版本。为了验证 v2 新版本，我们通常会选择先上线一个服务 B 的 v2 版本的新实例，通过 Istio 进行流量百分比拆分，比如切 1% 的流量到新的 v2 版本的，这被称为“灰度发布”。此时新的“切 1% 流量到 v2”的 CRD 被下发到服务 A 的 Sidecar，这 100 个 Sidecar 中的每个都会执行该灰度策略。如果 v2 版本有问题不能正常工作，则只影响到 1% 的流量，即此时 Istio 的灰度控制的是 CRD 配置生效之后 Sidecar 的流量控制行为。 但是，实际生产中，配置本身也是有风险的。假设在配置 Istio CRD 时出现低级错误，不小心将新旧版本的流量比例配反了，错误配置成了 99% 的流量去 v2 版本。则当新的 CRD 配置被下发到全部 100 个服务 A 的实例时并生效时， Sidecar 控制的流量就会发生非常大的变化，造成生产事故。 为了规避这个风险，就必须引入配置变更的范围控制，比如将新的 CRD 配置下发到少数 Sidecar，验证配置无误后再扩展到其他 Sidecar。 ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:3:4","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"应用平滑迁移的终极方案 在 Service Mesh 落地的过程中，现有应用如何平滑迁移到 Service Mesh，是一个至关重要的话题。典型如基于传统微服务框架如 SpringCloud/Dubbo 的应用，如何逐个（或者分批）的迁移到 Service Mesh 上。 蚂蚁金服在去年进行落地实践时，就特别针对应用平滑迁移进行了深入研究和探索。这个问题是 Service Mesh 社区非常关注的核心落地问题，今天我们重点分享。 在今年 9 月份的云栖大会上，蚂蚁金服推出了双模微服务的概念，如下图所示： “双模微服务”是指传统微服务和 Service Mesh 双剑合璧，即“基于 SDK 的传统微服务”可以和“基于 Sidecar 的 Service Mesh 微服务”实现下列目标： 互联互通：两个体系中的应用可以相互访问； 平滑迁移：应用可以在两个体系中迁移，对于调用该应用的其他应用，做到透明无感知； 灵活演进：在互联互通和平滑迁移实现之后，我们就可以根据实际情况进行灵活的应用改造和架构演进； 双模还包括对应用运行平台的要求，即两个体系下的应用，既可以运行在虚拟机之上，也可以运行在容器 /k8s 之上。 怎么实现这么一个美好的双模微服务目标呢？ 我们先来分析一下传统微服务体系和 Service Mesh 体系在服务注册 / 服务发现 / 服务相关的配置下发上的不同。 首先看传统微服务体系，其核心是服务注册中心 / 配置中心，应用通过引用 SDK 的方式来实现对接各种注册中心 / 配置中心。通常不同的注册中心 / 配置中心都有各自的实现机制和接口协议，SDK 和注册中心 / 配置中心的交互方式属于内部实现机制，并不通用。 优点是支持海量数据（十万级别甚至百万级别），具备极强的分发能力，而且经过十余年间的打磨，稳定可靠可谓久经考验。市面上有很多成熟的开源产品，各大公司也都有自己的稳定实现。如阿里集团的 Nacos，蚂蚁金服的 SOFARegistry。 SOFARegistry： https://github.com/sofastack/sofa-registry 缺点是注册中心 / 配置中心与 SDK 通常是透传数据，即注册中心 / 配置中心只进行数据的存储和分发。大量的控制逻辑需要在 SDK 中实现，而 SDK 是嵌入到应用中的。因此，任何变更都需要改动 SDK 并要求应用升级。 再来看看 Service Mesh 方案，以 Istio 为例： Service Mesh 的优点是引入了控制平面（在 Istio 中具体指 Pilot 组件），通过控制平面来提供强大的控制逻辑。而控制平面的引入，MCP/xDS 等标准协议的制订，实现了数据源和下发数据的解耦。即存储于注册中心 / 配置中心（在 Istio 中体现为 k8s api server + Galley）的数据可以有多种灵活的表现形式，如 CRD 形式的 Istio API，通过运行于 Pilot 中的 Controller 来实现控制逻辑和格式转换，最后统一转换到 xDS/UDPA。这给 API 的设计提供了非常大的施展空间，极具灵活度，扩展性非常好。 缺点也很明显，和成熟的注册中心 / 配置中心相比，支持的容量有限，下发的性能和稳定性相比之下有很大差距。 控制平面和传统注册中心 / 配置中心可谓各有千秋，尤其他们的优缺点是互补的，如何结合他们的优势？ 此外，如何打通两个体系是 Service Mesh 社区的老大难问题。 尤其是缺乏标准化的社区方案，只能自行其是，各自为战。 最近，在综合了过去一年多的思考和探索之后，蚂蚁金服和阿里集团的同事们共同提出了一套完整的解决方案，我们戏称为“终极方案”：希望可以通过这个方案打通传统微服务体系和 Service Mesh 体系，彻底终结这个困扰已久的问题。 这个方案的核心在于：以 MCP 和 xDS/UDPA协议为基础，融合控制平面和传统注册中心 / 配置中心。 如上图所示，如果我们将融合控制平面和传统注册中心 / 配置中心而来的新的产品形态视为一个整体，则这个新产品形态的能力主要有三块： 传统注册中心的数据存储能力：支持海量数据； Service Mesh 控制平面的能力：解耦之后 API 设计的弹性和灵活度； 传统注册中心的分发能力：性能、速度、稳定性； 这个新的产品形态可以理解为“带控制平面的注册中心 / 配置中心”，或者“存储 / 分发能力加强版的控制平面”。名字不重要，重要的是各节点的通讯交互协议必须标准化： MCP 协议：MCP 协议是 Istio 中用 于 Pilot 和 Galley 之间同步数据的协议，源自 xDS 协议。我们设想通过 MCP 协议将不同源的注册中心集成起来，目标是聚合多注册中心的数据到 Pilot 中，实现打通异构注册中心（未来也会用于多区域聚合）。 xDS/UDPA 协议：xDS 协议源自 Envoy，是目前数据平面的事实标准，UDPA 是正在进行中的基于 xDS 协议的标准化版本。Sidecar 基于 xDS/UDPA 协议接入控制平面，我们还有进一步的设想，希望加强 SDK 方案，向 Istio 的功能靠拢，具体表现为 SDK 支持 xDS 协议（初期版本先实现最小功能集）。目标是希望在对接控制平面的前提下，应用可以在 Service Mesh 和 SDK 方案之间自由选择和迁移。 基于这个思路，我们给出如下图所示的解决方案，希望最大限度的整合传统微服务框架和 Service Mesh。其基本指导原则是：求同存异，保持兼容。 上图中，蓝色部分是通用的功能模块，我们希望可以和社区一起共建。红色部分是不兼容的功能模块，但是保持 API 兼容。 具体说，右边是各种注册中心（配置中心同理）： Galley 和底下的 k8s API Server 可以视为一个特殊的注册中心，这是 Istio 的官方方式； Nacos/SOFARegistry 是阿里集团和蚂蚁金服的注册中心，支持海量规模。我们计划添加 MCP 协议的支持，直接对接 Pilot； 其他的注册中心，也可以通过提供 MCP 协议支持的方式，接入到这个方案中； 对于不支持 MCP 的注册中心，可以通过开发一个 MCP Proxy 模块以适配器模式的方式间接接入。当然最理想的状态是出现成熟的通用开源方案来-统一解决，比如 Nacos Sync 有计划做类似的事情； 左边是数据平面： Service Mesh 体系下的 Sidecar（如 Envoy 和蚂蚁金服的 SOFAMosn）目前都已经支持 xDS/UDPA； 相对来说，这个方案中比较“脑洞”的是在 SDK 方案如 Spring Cloud/Dubbo/SOFARPC 中提供 xDS 的支持，以便对接到已经汇总了全局数据的控制平面。从这个角度说，支持 xDS 的 SDK 方案，也可以视为广义的数据平面。我们希望后面可以推动社区朝这个方向推进，短期可以先简单对接，实现 xDS 的最小功能集；长期希望 SDK 方案的功能能向 Istio 看齐，实现更多的 xDS 定义的特性； 这个方案对运行平台没有任何特别要求，只要网络能通，应用和各个组件可以灵活选择运行在容器（k8s）中或虚拟机中。 需要特别强调的是，这个方案最大的优点在于它是一个高度标准化的社区方案：通过 MCP 协议和 xDS 协议对具体实现进行了解耦和抽象，整个方案没有绑定到任何产品和供应商。因此，我们希望这个方案不仅仅可以用于阿里集团和蚂蚁金服，也可以用于整个 Istio 社区。阿里集团和蚂蚁金服目前正在和 Istio 社区联系，我们计划将这个方案贡献出来，并努力完善和加强 Pilot 的能力，使之能够满足我们上面提到的的美好愿景：融合控制平面和传统注册中心 / 配置中心的优点，打通传统微服务框架和 Service Mesh，让应用可以平滑迁移灵活演进。 希望社区认可这个方案的同学可以参与进来，和我们一起努力来建设和完善它。 ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:3:5","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"是否采用 Service Mesh 的建议 在过去一年间，这个问题经常被人问起。借这个机会，结合过去一年中的实践，以及相比去年此时更多的心得和领悟，希望可以给出一些更具参考价值的建议。 有没有短期急迫需求，通常取决于当前有没有迫切需要解决的痛点。 在 Service Mesh 的发展过程中，有两个特别清晰而直接的痛点，它们甚至对 Service Mesh 的诞生起了直接的推动作用： ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:4:0","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"多语言支持 这是 SDK 方案的天然局限，也是 Service Mesh 的天然优势。需要支持的编程语言越多，为每个编程语言开发和维护一套 SDK 的成本就越高，就有越多的理由采用 Service Mesh。 类库升级困难 同样，这也是 SDK 方案的天然局限，也是 Service Mesh 的天然优势（还记得前面那个不科学的 -7% 吗？）。SDK 方案中类库和业务应用打包在一起，升级类库就不得不更新整个业务应用，而且是需要更新所有业务团队的所有应用。在大部分公司，这通常是一个非常困难的事情，而且每次 SDK 升级都要重复一次这种痛苦。 而且，这两个痛点有可能会同时存在：有多个编程语言的类库需要升级版本…… 所以，第一个建议是先检查是否存在这两个痛点。 Service Mesh 的无侵入性，在老应用升级改造，尤其是希望少改代码甚至完全不改代码的情况下，堪称神器。 所以，第二个建议是，如果有老应用无改动升级改造的需求，对流量控制、安全、可观测性有诉求，则可以考虑采用 Service Mesh。 这个建议仅仅适用于技术力量相对薄弱的企业，这些企业普遍存在一个问题：技术力量不足，或者主要精力投放在业务实现，导致无力维护统一的技术栈，系统呈现烟囱式架构。 传统烟囱式架构的常见问题有： 重复建设，重复造轮子； 不同时期，不同厂商，用不同的轮子； 难以维护和演进，后续成本高昂； 掌控力不足，容易受制于人； 这种情况下，建议引入 Service Mesh 技术，通过 Service Mesh 将非业务逻辑从应用剥离并下沉的特性，来统一整个公司的技术栈。 特别需要强调的是，对于技术力量不足、严重依赖外包和采购的企业，尤其是银行 / 保险 / 证券类金融企业，引入 Service Mesh 会有一个额外的特殊功效，至关重要： 将乙方限制在业务逻辑的实现上 即企业自行建设和控制 Service Mesh，作为统一的技术栈，在其上再开发运行业务应用。由于这些业务应用运行在 Servcie Mesh 之上，因此只需要实现业务逻辑，非业务逻辑的功能由 Servcie Mesh 来提供。通过这种方式，可以避免乙方公司借项目机会引入各种技术栈而造成技术栈混乱，导致后期维护成本超高；尤其是要避免引入私有技术栈，因为私有技术栈会造成对甲方事实上的技术绑定（甚至技术绑架）。 最后一个建议，和云原生有关。在去年的 QCon 演讲中，我曾经提到我们在探索 Kubernetes / Service Mesh / Serverless 结合的思路。在过去一年，蚂蚁金服一直在云原生领域做深度探索，也有一些收获。其中，有一点我们是非常明确的：Mesh 化是云原生落地的关键步骤。 下图展示了蚂蚁金服在云原生落地方面的基本思路： 最下方是云，以 Kubernetes 为核心，关于这一点社区基本已经达成共识：Kubernetes 就是云原生下的操作系统； 在 Kubernetes 之上，是 Mesh 层。不仅仅有我们熟悉的 Service Mesh，还有诸如 Database Mesh 和 Message Mesh 等类似的其他 Mesh 产品形态，这些 Mesh 组成了一个标准化的通信层； 运行在各种 Mesh 的应用，不管是微服务形态，还是传统非微服务形态，都可以借助 Mesh 的帮助实现应用轻量化，非业务逻辑的各种功能被剥离到 Mesh 中后，应用得以“瘦身减负”； 瘦身之后的应用，其内容主要是业务逻辑实现。这样的工作负载形式，更适合 Serverless 的要求，为接下来转型 Serverless 做好准备； 所以，我的最后一个建议是，请结合你的长远发展方向考虑：如果云原生是你的诗和远方，那么 Service Mesh 就是必由之路。 Kubernetes / Service Mesh / Serverless 是当下云原生落地实践的三驾马车，相辅相成，相得益彰。 在最后，重申一下 Service Mesh 的核心价值： 实现业务逻辑和非业务逻辑的分离。 前面的关于要不要采用 Service Mesh 四个建议，归根到底，最终都是对这个核心价值的延展。只有在分离业务逻辑和非业务逻辑并以 Sidecar 形式独立部署之后，才有了这四个建议所依赖的特性： Service Mesh 的多语言支持和应用无感知升级； 无侵入的为应用引入各种高级特性如流量控制，安全，可观测性； 形成统一的技术栈； 为非业务逻辑相关的功能下沉到基础设施提供可能，帮助应用轻量化，使之专注于业务，进而实现应用云原生化； 希望大家在理解 Service Mesh 的核心价值之后，再来权衡要不要采用 Service Mesh，也希望我上面给出的四个建议可以对大家的决策有所帮助。 ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:4:1","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"总 结 在今天的内容中，首先介绍了蚂蚁金服 Service Mesh 的发展历程，给大家展示了双十一大规模落地的规模和性能指标，并解释了这些指标背后的原理。然后分享了蚂蚁金服在 Service Mesh 大规模落地中遇到的困难和挑战，以及我们为此做的工作，重点介绍了应用平滑迁移的所谓“终极方案”；最后结合蚂蚁金服在云原生和 Service Mesh 上的实践心得，对于是否应该采用 Service Mesh 给出了几点建议。 目前蚂蚁金服正在静待今年的双十一大考，这将是 Service Mesh 的历史时刻：全球最大规模的 Service Mesh 集群，Service Mesh 首次超大规模部署…… 一切都是如此的值得期待。 请对 Service Mesh 感兴趣的同学稍后继续关注，预期在双十一之后会有一系列的分享活动： 经验分享：会有更多的技术分享，包括落地场景，经验教训，实施方案，架构设计… 开源贡献：蚂蚁金服会将落地实践中的技术实现和方案以不同的方式回馈社区，推动 Service Mesh 落地实践。目前这个工作正在实质性的进行中， 请留意我们稍后公布的消息； 商务合作：蚂蚁金服即将推出 Service Mesh 产品，提供商业产品和技术支持，提供金融级特性，欢迎联系； 社区交流：ServiceMesher 技术社区继续承担国内 Service Mesh 布道和交流的重任；欢迎参加我们今年正在持续举办的 Service Mesh Meetup 活动。 今年是我在 QCon 演讲的第三年，这三年中的三次演讲，可以说是从一个侧面反映了国内 Service Mesh 发展的不同阶段： 2017 年，国内 Service Mesh 一片蛮荒的时候，我做了 Service Mesh 的布道，介绍了 Service Mesh 的原理，喊出了“下一代微服务”的口号 ; 2018 年，以蚂蚁金服为代表的国内互联网企业，陆陆续续开始了 Service Mesh 的落地探索，所谓摸着石头过河不外如是。第二次演讲我分享了蚂蚁金服的探索性实践，介绍了蚂蚁金服的 Service Mesh 落地方式和思路。 今天，2019 年，第三次演讲，蚂蚁金服已经建立起了全球最大规模的 Service Mesh 集群并准备迎接双十一的严峻挑战，这次的标题也变成了深度实践。 从布道，到探索，再到深度实践，一路走来已是三年，国内的 Service Mesh 发展，也从籍籍无名，到炙手可热，再到理性回归。Service Mesh 的落地，依然还存在非常多的问题，距离普及还有非常远的路要走，然而 Service Mesh 的方向，已经被越来越多的人了解和认可。 高晓松说：“生活不止眼前的苟且，还有诗和远方”。对于 Service Mesh 这样的新技术来说，也是如此。 鸣谢 InfoQ 和 Qcon 提供的机会，让我得以每年一次的为大家分享 Service Mesh 的内容。2020 年，蚂蚁金服将继续推进和扩大 Service Mesh 落地的规模，继续引领 Service Mesh 在金融行业的实践探索。希望明年，可以有更多更深入的内容带给大家！ ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:5:0","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"作者介绍 敖小剑，蚂蚁金服高级技术专家，十七年软件开发经验，微服务专家，Service Mesh 布道师，ServiceMesher 社区联合创始人。专注于基础架构和中间件，Cloud Native 拥护者，敏捷实践者，坚守开发一线打磨匠艺的架构师。曾在亚信、爱立信、唯品会等任职，目前就职蚂蚁金服，在中间件团队从事 Service Mesh/ Serverless 等云原生产品开发。本文整理自 10 月 18 日在 QCon 上海 2019 上的演讲内容。 ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:5:1","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["Service Mesh"],"content":"个人理解 我在一家创业公司的时候有过多语言的经历，当时主要 Web 端有 Java 和 Go，其它还有 Python 和 Php的，每次改动需要更新各个语言的SDK，当时就在想我们难道不能有一个跨语言的一站式方案吗。 我在做偏老派的 Java 开发的时候特别烦的一件事情就是各种 SDK ，如果 SDK 有漏洞重新发布需要通知到各个服务 Owner 更改，人少的时候吼一声可能就解决了，等团队的人越来越多，即使在群里 @ 了大家，也不一定能普及到，你可能需要一个一个去确认，往往需要折腾个几天才改完，低效乏味。 经历过痛苦的人才会更加憧憬美好 这篇文章给了我非常大的感触，知道小剑也有一年多了，真心羡慕走在前沿的大佬。 我从15年认识了 Java，到17年认识了 docker 和 kubernetes， 给了我巨大的冲击， 18年认识了 服务网格，servless，听到云原生的赞歌，现在我后悔了，我只是一个 Java 开发让我不满，告诫自己保持一颗热爱的心，希望自己在2020年能和云原生牵上线。 ","date":"2019-11-07","objectID":"/2019/11/server-mesh-ant/:5:2","tags":["转载","云原生"],"title":"【转载】蚂蚁金服 Service Mesh 深度实践","uri":"/2019/11/server-mesh-ant/"},{"categories":["设计模式"],"content":"本文介绍在 go 语言中使用装饰者模式. ","date":"2019-11-04","objectID":"/2019/11/pattern-decorator/:0:0","tags":["golang","装饰者模式"],"title":"装饰者模式","uri":"/2019/11/pattern-decorator/"},{"categories":["设计模式"],"content":"一、前言 Decorator Pattern（装饰模式） 维基百科 This pattern creates a decorator class which wraps the original class and provides additional functionality keeping class methods signature intact. 白话文: 这个模式我们需要创建一个新的装饰类,然后装饰类会拥有一个属性是被装饰类,并且会有“一个”方法和需要使用的被装饰类的方法签名完全一致,调用装饰类的方法会执行装饰类内容并调用被装饰类的被装饰方法. 故事: 我买了一本书《go编程思想》,小明也买了一本书《go编程思想》并给它带上了一个黄金封面,两本书内容一摸一样,只是小明的变成金灿灿的土豪版. ","date":"2019-11-04","objectID":"/2019/11/pattern-decorator/:1:0","tags":["golang","装饰者模式"],"title":"装饰者模式","uri":"/2019/11/pattern-decorator/"},{"categories":["设计模式"],"content":"二、实例 ","date":"2019-11-04","objectID":"/2019/11/pattern-decorator/:2:0","tags":["golang","装饰者模式"],"title":"装饰者模式","uri":"/2019/11/pattern-decorator/"},{"categories":["设计模式"],"content":"2.1 对于某个接口的方法装饰 代码: package decorator import ( \"fmt\" \"testing\" ) type Shape interface { draw() } type Circle struct { } func (shape Circle) draw() { fmt.Println(\"Shape: Circle\") } type Rectangle struct { } func (shape Rectangle) draw() { fmt.Println(\"Shape: Rectangle\") } type ShapeDecorator struct { decoratorShape Shape } func (shape ShapeDecorator) draw() { shape.decoratorShape.draw() } type RedShapeDecorator struct { ShapeDecorator } func NewRedShapeDecorator(s Shape) *RedShapeDecorator { d := new(RedShapeDecorator) d.decoratorShape = s return d } func (shape RedShapeDecorator) draw() { shape.ShapeDecorator.draw() fmt.Println(\"red\") } type BlueShapeDecorator struct { ShapeDecorator } func NewBlueShapeDecorator(s Shape) *BlueShapeDecorator { d := new(BlueShapeDecorator) d.decoratorShape = s return d } func (shape BlueShapeDecorator) draw() { shape.ShapeDecorator.draw() fmt.Println(\"blue\") } func TestName(t *testing.T) { redShapedDecorator := NewRedShapeDecorator(Circle{}) redShapedDecorator.draw() redShapedDecorator = NewRedShapeDecorator(Rectangle{}) redShapedDecorator.draw() blueShapedDecorator := NewBlueShapeDecorator(Circle{}) blueShapedDecorator.draw() blueShapedDecorator = NewBlueShapeDecorator(Rectangle{}) blueShapedDecorator.draw() } 输出结果: Shape: Circle red Shape: Rectangle red Shape: Circle blue Shape: Rectangle blue ","date":"2019-11-04","objectID":"/2019/11/pattern-decorator/:2:1","tags":["golang","装饰者模式"],"title":"装饰者模式","uri":"/2019/11/pattern-decorator/"},{"categories":["设计模式"],"content":"2.2 直接装饰方法 代码: type drawFunc func() func CircleDraw() { fmt.Println(\"Shape: Circle\") } func RedCircleDraw(d drawFunc) drawFunc { return func() { d() fmt.Println(\"red\") } } func TestFunc(t *testing.T) { var drawFunc drawFunc drawFunc = CircleDraw drawFunc() drawFunc = RedCircleDraw(drawFunc) drawFunc() } 输出结果: Shape: Circle Shape: Circle red ","date":"2019-11-04","objectID":"/2019/11/pattern-decorator/:2:2","tags":["golang","装饰者模式"],"title":"装饰者模式","uri":"/2019/11/pattern-decorator/"},{"categories":["设计模式"],"content":"三、参考 https://www.tutorialspoint.com/design_pattern/decorator_pattern.htm ","date":"2019-11-04","objectID":"/2019/11/pattern-decorator/:3:0","tags":["golang","装饰者模式"],"title":"装饰者模式","uri":"/2019/11/pattern-decorator/"},{"categories":["阿里云"],"content":"本文介绍阿里云数据库 binglog 配置注意点. ","date":"2019-09-22","objectID":"/2019/09/aliyun-daily-01/:0:0","tags":["RDS","MySQL","binlog","otter","canal"],"title":"阿里云采坑记录 | RDS 的 binglog 丢失","uri":"/2019/09/aliyun-daily-01/"},{"categories":["阿里云"],"content":"一、前言 上周同事负责的同步服务出现宕机后,由于在忙于另一个重要的项目,线上没有及时处理,后发现同步数据丢失.我趁机了解了下我们的同步逻辑并对这次异常做一个简单的总结. 情况描述: 线上我们基于 otter 的 msyql 数据同步服务出错,出错后会停止数据同步（我们后续的配置没有从中心节点同步到私有云节点）,导致了私有云无法正常启动部分服务. 我们分阿里云（中心节点）,北京私有云节点,广州私有云节点等,数据会从中心节点同步到私有云节点 中心节点使用了阿里云的RDS MySQL数据库,私有云节点采用自己搭建的MySQL 采用基于otter的数据同步服务（otter基于canal） 我们采用了xxl-job来做定时调度,因为之前认为它只有DML操作,我们的同步服务没有对它的DDL操作做处理 xxl-job的机器配置的较低,数据量变大之后XXL_JOB_QRTZ_TRIGGER_LOG的查询语句运行较慢,我们给它增加了个索引 我们的同步服务异常后,会停止数据同步 A服务发布,在中心节点加了配置,私有云节点启动失败 查询发现同步服务异常,无法通过binlog的偏移量找到记录,导致无法把中心节点加了的配置同步到私有云节点 我们是在一天后对这个问题做的处理 ","date":"2019-09-22","objectID":"/2019/09/aliyun-daily-01/:1:0","tags":["RDS","MySQL","binlog","otter","canal"],"title":"阿里云采坑记录 | RDS 的 binglog 丢失","uri":"/2019/09/aliyun-daily-01/"},{"categories":["阿里云"],"content":"二、排查 ","date":"2019-09-22","objectID":"/2019/09/aliyun-daily-01/:2:0","tags":["RDS","MySQL","binlog","otter","canal"],"title":"阿里云采坑记录 | RDS 的 binglog 丢失","uri":"/2019/09/aliyun-daily-01/"},{"categories":["阿里云"],"content":"2.1 xxl job的变化 xxl-job的SQL: SELECT t.id, t.job_group, t.job_id, t.executor_address, t.executor_handler , t.executor_param, t.executor_sharding_param, t.executor_fail_retry_count, t.trigger_time, t.trigger_code , t.trigger_msg, t.handle_time, t.handle_code, t.handle_msg FROM XXL_JOB_QRTZ_TRIGGER_LOG t WHERE t.job_group = ? AND t.job_id = ? AND t.trigger_time \u003e= ? AND t.trigger_time \u003c= ? ORDER BY id DESC LIMIT ?, ? 并没有异常情况. ","date":"2019-09-22","objectID":"/2019/09/aliyun-daily-01/:2:1","tags":["RDS","MySQL","binlog","otter","canal"],"title":"阿里云采坑记录 | RDS 的 binglog 丢失","uri":"/2019/09/aliyun-daily-01/"},{"categories":["阿里云"],"content":"2.2 MySQL 排查 mysql 查看 binlog: // 查看binlog文件列表 mysql\u003e show binary logs; +------------------+-----------+ | Log_name | File_size | +------------------+-----------+ | mysql-bin.000001 | 107853 | +------------------+-----------+ 1 row in set (0.00 sec) // 查看binlog状态 mysql\u003e show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000001 | 107853 | | | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 并没有异常情况. ","date":"2019-09-22","objectID":"/2019/09/aliyun-daily-01/:2:2","tags":["RDS","MySQL","binlog","otter","canal"],"title":"阿里云采坑记录 | RDS 的 binglog 丢失","uri":"/2019/09/aliyun-daily-01/"},{"categories":["阿里云"],"content":"三、解决方案 因为同步服务是由于binlog的偏移量问题而失败,偏移量是通过zk节点去获取的,我们去MySQL查询了最新的可用的偏移量,设置到了zk的指定节点,让同步服务正常运行.（我们的配置同步在新偏移量之后,所以启动后能够从中心节点同步到私有云节点） 这个是临时的解决方案,我们没法找到之前的binlog的完整记录 ","date":"2019-09-22","objectID":"/2019/09/aliyun-daily-01/:3:0","tags":["RDS","MySQL","binlog","otter","canal"],"title":"阿里云采坑记录 | RDS 的 binglog 丢失","uri":"/2019/09/aliyun-daily-01/"},{"categories":["阿里云"],"content":"四、思考问题 ","date":"2019-09-22","objectID":"/2019/09/aliyun-daily-01/:4:0","tags":["RDS","MySQL","binlog","otter","canal"],"title":"阿里云采坑记录 | RDS 的 binglog 丢失","uri":"/2019/09/aliyun-daily-01/"},{"categories":["阿里云"],"content":"4.1 为什么过了一天,同步服务会启动失败？ 同步服务失败,因为binlog找不到.MySQL我们自己安装的话binlog配置默认是不清理的,但是RDS上不是这样的. 下面是RDS默认配置: 保留时长:默认值为18,表示实例空间内默认保存最近18个小时内的Binlog文件,18个小时之前的日志将在备份后（需要开启日志备份）清理.保留时长可选范围值为0~7*24小时. 空间使用率不超过:默认值为30%,表示本地Binlog空间使用率大于30%时,系统会从最早的Binlog开始清理,直到空间使用率低于30%.空间使用率不超过可选范围值为0 - 50% . 可用空间保护,默认开启该功能,表示当实例总空间使用率超过80%或实例剩余可用空间不足5GB时,会强制从最早的Binlog开始清理,直到总空间使用率降到80%以下且实例剩余可用空间大于5GB. 我们可以看到RDS默认保留时间小于一天,所以我们停了一天后再度开启,导致binlog位置找不到,只能从最新的偏移量同步.这里首先把保留时长调至3天（我们的同步服务不可能停3天,在某些改造项目,同步服务可能会停1-2天）,这个需要根据实际场景去设置合理的值. ","date":"2019-09-22","objectID":"/2019/09/aliyun-daily-01/:4:1","tags":["RDS","MySQL","binlog","otter","canal"],"title":"阿里云采坑记录 | RDS 的 binglog 丢失","uri":"/2019/09/aliyun-daily-01/"},{"categories":["阿里云"],"content":"4.2 binlog不在了,如何补救？ 阿里云会把binlog保存到OSS,从OSS下载回来binlog,然后把binlog设置到MySQL指定位置（这块可能是我脑补的）.这块有任何问题可以提工单,毕竟顾客是“上帝”. 本地验证的时候,位置是: /usr/local/mysql/data ","date":"2019-09-22","objectID":"/2019/09/aliyun-daily-01/:4:2","tags":["RDS","MySQL","binlog","otter","canal"],"title":"阿里云采坑记录 | RDS 的 binglog 丢失","uri":"/2019/09/aliyun-daily-01/"},{"categories":null,"content":"一个兴趣使然的开发人员 语言技能：Java，Golang 技术方向：微服务，云原生，领域驱动 联系我： 邮箱：tczjhz@163.com GitHub：cityiron ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"","uri":"/about/"}]